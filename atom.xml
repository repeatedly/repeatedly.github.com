<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Go ahead!]]></title>
  <link href="http://repeatedly.github.com/atom.xml" rel="self"/>
  <link href="http://repeatedly.github.com/"/>
  <updated>2014-09-22T01:42:21+09:00</updated>
  <id>http://repeatedly.github.com/</id>
  <author>
    <name><![CDATA[Masahiro Nakagawa]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[ServerEngine at RubyKaigi 2014]]></title>
    <link href="http://repeatedly.github.com/ja/2014/09/serverengine-at-rubykaigi-2014/"/>
    <updated>2014-09-22T00:03:00+09:00</updated>
    <id>http://repeatedly.github.com/ja/2014/09/serverengine-at-rubykaigi-2014</id>
    <content type="html"><![CDATA[<p><a href="http://rubykaigi.org/2014">RubyKaigi 2014</a>で<a href="http://rubykaigi.org/2014/presentation/S-MasahiroNakagawa">ServerEngineについて発表</a>してきました．</p>

<p>最初は<a href="http://www.fluentd.org/">Fluentd</a>で発表しようかと思ったんですが，別の有用なプロジェクトの話もそろそろした方がいいかな，ということでServerEngineにしました． <a href="http://blog.livedoor.jp/sonots/archives/40303560.html">@sonotsさんがFluentdの発表をしてくれた</a>ので，被らなくて良かった…</p>

<p><img src="http://repeatedly.github.com/images/rubykaigi2014_serverengine_pic.jpg" width="425" title="&#34;RubyKaigi 2014: ServerEngine&#34;" alt="&#34;RubyKaigi 2014: ServerEngine&#34;">
画像は<a href="https://twitter.com/gihyoreport/status/512867195504701440/photo/1">技評さんから</a>．</p>

<p>以下がスライドです．書いてないことも発表では色々と話したので，動画が公開されればそれとセットで見た方が良いです．</p>

<iframe src="http://repeatedly.github.com//www.slideshare.net/slideshow/embed_code/39292931" width="425" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="http://repeatedly.github.com//www.slideshare.net/treasure-data/rubykaigi-2014-serverengine" title="RubyKaigi 2014: ServerEngine" target="_blank">RubyKaigi 2014: ServerEngine</a> </strong> from <strong><a href="http://repeatedly.github.com//www.slideshare.net/treasure-data" target="_blank">Treasure Data, Inc.</a></strong> </div></p>

<h2>ServerEngine</h2>

<p><a href="https://github.com/fluent/serverengine">fluent/serverengine</a></p>

<p>ServerEngineはTreasure Dataで開発・運用されている分散キューや分散スケジューラ，それとFluentdなどの経験を元に，汎用的な部分を抽出してフレームワークにしたプロダクトです．発表で言及した機能の他にもBlockingFlagなどのユーティリティがあるので，Rubyでデーモンやバッチワーカーを書くときには是非試して貰えればと思います．</p>

<p>また，FluentdもいずれはServerEngineベースになる予定です．今でもServerEngineと似たような機能を俺俺で実装しているのですが，ServerEngineにすることでよりコードは簡潔に，また新しい機能も実装出来るようになります．この辺は乞うご期待という感じです．</p>

<h2>発表に関して</h2>

<p>1週間前に台北で英語でのキーノートがあり，直前まで時間が取れませんでした．台北後にひたすらスライドを作り，前日にやっと完成という，かなり際どいスケジュール．</p>

<p>本番どうかなと思ったんですが，時間進行を見ながら一部詳細を飛ばしたり，メインホールだったけどそんなに詰まることなく発表出来たんじゃないかなと思います．ここ数年は色々と大きいイベントでの発表も増え，それらの経験のおかげか，日本語であればそれなりにこなせるようになってきたかなと実感してます．</p>

<p>また，Railsとか関係のない，どちらかというと地味というか泥臭い感じの話が多めだったんですが，何人かには受けたようで，良かったです．</p>

<blockquote class="twitter-tweet" lang="en"><p><a href="https://twitter.com/repeatedly">@repeatedly</a> 今年のセッションの中でトップクラスなぐらい良いセッションでした</p>&mdash; 複数DB (@ryopeko) <a href="https://twitter.com/ryopeko/status/512867029565460480">September 19, 2014</a></blockquote>


<script async src="http://repeatedly.github.com//platform.twitter.com/widgets.js" charset="utf-8"></script>




<blockquote class="twitter-tweet" data-conversation="none" lang="en"><p><a href="https://twitter.com/repeatedly">@repeatedly</a> よかったですよ！数年前にあれば、あの時苦労しなくてもよかったのに…と思ってしまいました</p>&mdash; Toshiwo (@toshiwo) <a href="https://twitter.com/toshiwo/status/512879201326596097">September 19, 2014</a></blockquote>


<script async src="http://repeatedly.github.com//platform.twitter.com/widgets.js" charset="utf-8"></script>




<blockquote class="twitter-tweet" data-conversation="none" lang="en"><p><a href="https://twitter.com/repeatedly">@repeatedly</a> BTW, your talk was great and ServerEngine is cool. Thanks!</p>&mdash; Hiroshi Nakamura (@nahi) <a href="https://twitter.com/nahi/status/513213969855565824">September 20, 2014</a></blockquote>


<script async src="http://repeatedly.github.com//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>聞きに来てくれた皆さん，手伝ってくれたスタッフ，そして同時通訳の方々，ありがとうございました！！</p>

<p>とりあえず8月9月のイベントラッシュが終わった感じなので，少しのんびりしたい…んだけど，11月のRubyConfで<a href="http://rubyconf.org/program#prop_569">kiyoto-sanがFluentdについて発表する</a>ようなので，俺も参加しているかもしれません．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[YAPC::Asia 2014]]></title>
    <link href="http://repeatedly.github.com/ja/2014/08/yapc-asia-2014/"/>
    <updated>2014-08-31T11:59:00+09:00</updated>
    <id>http://repeatedly.github.com/ja/2014/08/yapc-asia-2014</id>
    <content type="html"><![CDATA[<p><a href="http://yapcasia.org/2014/">YAPC::Asia 2014</a>に初めて参加した．</p>

<p>Perl使ってないし最初は行く気なかったんだけど，登録中のセッション見るとPerlじゃないの多いし，
個人スポンサーになればTシャツやパーカーも貰えると言うことで，個人スポンサーでチケット入手．</p>

<p>0日目は参加せず，1日目は朝から，2日目は昼から参加．</p>

<h2>1日目</h2>

<h3>完成されたシステムなどない。完成された人間もいない。あるのは成長し続ける未完成なシステムと、それを支える未完成な人間だけだ</h3>

<p>YAPCでのTEDを目指したという発表．大ホールの暗い雰囲気と，宇宙に覆われたスライドによって，
宗教的な感じがしたプレゼン．</p>

<p>発表時間の半分で発表そのものは終わったのだけど，質疑応答をもっと盛り上げれば，完成度は上がったんじゃ無いかと思う．</p>

<h3>お待たせしました。Perl で BDD を簡単に実践する最高にクールなフレームワークができました</h3>

<p>一番Perlっぽい話かな，と思って参加．Perlのテストにおける歴史みたいなのとか，
エコシステムの話みたいなのが聞けたのは面白かった．</p>

<p>「Perl 6と同じくらい夢が詰まっている」という言葉，つらい．</p>

<h3>作られては消えていく、泡のように儚いクラスタの運用話</h3>

<p>TV番組用システムの裏側の話．あるある系もあったり，便利そうなコマンド作ってたり，
なかなかみんな苦労しているな，と．最近TV系の運用話はちょくちょく聞くので，どこも似ている感じ．</p>

<h3>ライブコーディング</h3>

<p>songmuさんともずにおんのコンビ漫才を休憩がてら見ていた．やっぱ本番では色々とミスが起きるｗ</p>

<p>ここで食べたDMMかき氷，普通に美味しかった</p>

<h3>O2O/IoT/Wearable時代におけるWeb以外のネットワーク技術入門</h3>

<p>IoT周辺の技術の用語とか仕組みの説明．いやー，しかしこの辺りは略語が多くてすぐ覚えるのがつらい．</p>

<p>その後はセッションには参加せず，休憩スペースで何人かでISUCONとかPHPとか色々とだべっていた．
MQTT Meetupに参加する予定があったので，懇親会には参加せず会場を去った．</p>

<h2>2日目</h2>

<p>naoyaさんの発表をにやにやしながら見ようかと思っていたのだけど，起きたらもう終わっていたのが心残り．</p>

<p>ついた時間も中途半端だったので，naoyaさんとHUBに行き，そこでmiyagawaさんとかけんじおじさん，shiba_yuさんとかとしゃべっていた．
時間が経つと色々と人が集まってきたのだけど，大ホールの席を取るため移動．</p>

<h3>そんなにビッグでもないデータ処理手法の話</h3>

<p>言うことはないというか，全部知っている話だったので内職をしていた．が，ベストトークで2位だったので，
やっぱ分野が違うと受けるトークも変わるんだなぁという．</p>

<p>ただ，最初にモリスさんがFluentdのユーザを聞いてくれたら半分くらい？は手をあげていたので，
ちゃんと浸透してるんだなと嬉しく思う反面，あと半数にどうやってリーチすべきかというのも考えないと行けない．</p>

<p>モリストリーム…じゃなくてNorikraの宣伝が少なかったな，という印象．</p>

<h3>Lightning Talks Day 2</h3>

<p>席を取っていて良かった，というくらい人がわらわらと集まってきていた．</p>

<p>FluentdがTwitterで非常にアクティブに議論・サポートされている話とか出てきて，Twiterの監視社会怖い!<br />
やっぱLTは短い中にネタを押し込んでくる人が多いので，それ故に面白いのだけど，すぐ終わるのがちょっと悲しい．</p>

<h3>キーノート</h3>

<p>typesterさんのエモい話．typesterさんのキャリアって他の人が真似するのはかなり難しいのだけど，
話の中でも出ていた「複数の人のロールモデルを参考にする」というのに結局落ち着く．</p>

<p>お子さんも一緒に来ていて，ほっこりする感じの発表だった．</p>

<p>あと，typesterさんはIngressは緑のLv8だったので，青のLv8の俺とどこかでやりあってたかも，
と思ったけど鎌倉だし全然関係なかった．</p>

<h3>HUB</h3>

<p>そうそうに一つのビールが無くなっていたけど，ジンジャーエール派の俺には関係なかった．
土曜日は夜に外せない用事があるので，色々な人に絡んでから，早めに帰った．</p>

<h2>改善点</h2>

<p>他の人も書いてると思うけど，意見が多いとそれだけ反映されるだろうということで書いておく．</p>

<h3>キャパシティプランニング</h3>

<p>今年の参加者は約1300人で，それに対して会場のキャパシティは約500人らしく，非常に狭かった．
特に多目的教室は毎度部屋から出るくらいまで立ち見とか出ていたので，ちゃんと聞けない人も多かったのではないかなと．</p>

<p>去年も同じ場所で約1100人だったらしいけど，去年は大丈夫だったのかな？参加してないのでよく分からない．
この辺のマネージは本当に難しいので，一発で解決のアイディアは提示できないのだけども…</p>

<h3>トーク間の移動時間</h3>

<p>2つのトークが続いて休憩がないのがいくつかあって，これは質疑応答と移動時間がかぶってしまうので勿体ないなと．</p>

<h2>まとめ</h2>

<p>トークの種類がPerlに限らず色々とあり，それに伴って参加者も結構多様な感じがしていて楽しかった．
ベストトーク賞のベスト3がどれもPerl関係ないのが，それを物語っていると思う．</p>

<p>ネットワークも快適だったし，休憩スペースも飲み物とかあって，セッション以外でぶらぶらするのも困らなかった．</p>

<p>1000人規模の運営超大変だし，運営チームの方達は本当にお疲れ様でした！来年も楽しみにしてます :)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fluentd v0.12でのFilterとLabel]]></title>
    <link href="http://repeatedly.github.com/ja/2014/08/fluentd-filter-and-label/"/>
    <updated>2014-08-25T17:30:00+09:00</updated>
    <id>http://repeatedly.github.com/ja/2014/08/fluentd-filter-and-label</id>
    <content type="html"><![CDATA[<p>Fluentd，最近だと海外でも露出が増えてきていて，軽量・柔軟・ロバストという所で，
新規の他，既存のログコレクタのリプレース含め，採用する所が増えてたりします．</p>

<p>より改善するため色々とユーザにヒアリングした結果，「フィルタ機能が欲しい」というのが一番多い意見でした．
Fluentdは元々Treasure Dataへロバストにデータを転送するためのミドルウェアで，「ETLとかはTreasure Dataで」
というのもあり，組み込みでフィルタ機能はありませんでした．</p>

<p>今現在のOutputプラグインによるフィルタ実装は，タグの書き換えが必要だったりして少し慣れが必要で，初心者にはちと難しい．
ということで，より簡単に効率よくデータストリームを扱えるフィルタ機能を入れることにしました！</p>

<p>前置きが長くなりましたが，次のバージョンであるv0.12ではFilterとLabelの導入が目玉機能になります．
これらは二つともデータストリームの処理をより楽にするための機能です．</p>

<p>後，<a href="http://www.fluentd.org/blog/fluentd-v0.10.53-is-released">Fluentd v0.10.53のリリースアナウンス</a>でも書きましたが，
今現在のmasterブランチはすでにv0.12用になっています．また，以下の機能はまだmasterにはマージされていません．今週中にはマージ予定ですが，
試して見たい方は，<a href="https://github.com/fluent/fluentd/pull/416">PR #416</a>をチェックしてください．</p>

<h2>Filter</h2>

<p>Outputプラグインで出力する前に，イベント群に処理を適用するための仕組みです．
<code>grep</code>や<code>record_refomer</code>や<code>geoip</code>プラグインなどは，Filterとして実装することでより効率良く処理出来るようになります．</p>

<p>最初に実装，その後に設定の例を見せます．</p>

<h3>実装</h3>

<p>FilterのAPIは以下のようになっています．</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
</pre></td><td class='code'><pre><code class='rb'><span class='line'><span class="k">module</span> <span class="nn">Fluent</span>
</span><span class='line'>  <span class="k">class</span> <span class="nc">SomeFilter</span> <span class="o">&lt;</span> <span class="no">Filter</span>
</span><span class='line'>    <span class="no">Plugin</span><span class="o">.</span><span class="n">register_filter</span><span class="p">(</span><span class="s1">&#39;some_filter&#39;</span><span class="p">,</span> <span class="nb">self</span><span class="p">)</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">def</span> <span class="nf">filter</span><span class="p">(</span><span class="n">tag</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">record</span><span class="p">)</span>
</span><span class='line'>      <span class="c1"># レコードを弄るプラグインは，ここを実装すればOK</span>
</span><span class='line'>      <span class="c1"># レコードを返す必要がある</span>
</span><span class='line'>      <span class="n">modified_record</span>
</span><span class='line'>    <span class="k">end</span>
</span><span class='line'>
</span><span class='line'>    <span class="k">def</span> <span class="nf">filter_stream</span><span class="p">(</span><span class="n">tag</span><span class="p">,</span> <span class="n">es</span><span class="p">)</span>
</span><span class='line'>      <span class="c1"># 以下はFilterのデフォルト実装．grepなどはこちらを置き換えればOK</span>
</span><span class='line'>      <span class="n">new_es</span> <span class="o">=</span> <span class="no">MultiEventStream</span><span class="o">.</span><span class="n">new</span>
</span><span class='line'>      <span class="n">es</span><span class="o">.</span><span class="n">each</span> <span class="p">{</span> <span class="o">|</span><span class="n">time</span><span class="p">,</span> <span class="n">record</span><span class="o">|</span>
</span><span class='line'>        <span class="n">new_es</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">filter</span><span class="p">(</span><span class="n">tag</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">record</span><span class="p">))</span>
</span><span class='line'>      <span class="p">}</span>
</span><span class='line'>      <span class="n">new_es</span>
</span><span class='line'>    <span class="k">end</span>
</span><span class='line'>  <span class="k">end</span> <span class="k">if</span> <span class="n">defined?</span><span class="p">(</span><span class="no">Filter</span><span class="p">)</span> <span class="c1"># v0.10とかでエラーにならないための回避策</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure>


<h4>filter(tag, time, record)</h4>

<p><code>SetXXXMixin</code>を置き換える<a href="https://gist.github.com/repeatedly/cb16d5667350c8a0e2c9#file-filter_add_metadata-rb">add_metadata</a>フィルタを見て貰えればわかりやすいと思いますが，
引数で渡ってくるレコードを弄って，それを返り値にするだけです<br />
<code>filter</code>とは違い<code>filter_stream</code>は上記のデフォルト実装があるので，変更する必要はありません．</p>

<p>大抵のプラグインはこのAPIをオーバーライドして処理を実装すれば，その結果が次のフィルタに渡されるようになります．</p>

<h4>filter_stream(tag, es)</h4>

<p>EventStreamを直接処理するFilterを書くためのAPIです．例えば<code>grep</code>はレコードを弄らず，EventStream全体に対して処理を行うプラグインなので，
このAPIをオーバーライドします．<code>filter_stream</code>を変更する場合，<code>filter</code>を変更する必要はありません．<br />
<code>filter</code>とは違い，返り値としてEventStreamを返す必要があります．Fluentdは，この<code>filter_stream</code>の返り値を次のFilterに渡し，最終結果をOutputプラグインに渡します．</p>

<p>また，<code>record_reformer</code>の<code>renew_record</code>のように，新しくレコードを生成する系のプラグインもこちら側で処理を実装することになります．
<code>grep</code>と<code>record_reformer</code>のFilter例は以下にあります．</p>

<ul>
<li><a href="https://gist.github.com/repeatedly/0ed3e2b4016b4045640a#file-filter_grep-rb">filter_grep</a></li>
<li><a href="https://gist.github.com/repeatedly/5c9f89e14aace5cda195#file-filter_record_reformer-rb">filter_record_reformer</a></li>
</ul>


<h3>設定</h3>

<p><code>&lt;filter&gt;</code>セクションが追加されています．たとえば，<code>add_metadata</code>と<code>grep</code>を使う場合には以下のようになります．</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='aconf'><span class='line'><span class="nt">&lt;source&gt;</span>
</span><span class='line'>  <span class="nb">type</span> forward
</span><span class='line'><span class="nt">&lt;/source&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;filter&gt;</span> <span class="c"># &lt;filter&gt;と&lt;filter **&gt;は同じ</span>
</span><span class='line'>  <span class="nb">type</span> grep
</span><span class='line'>  <span class="err">input_</span><span class="nb">key</span> message
</span><span class='line'>  <span class="nb">regexp</span> <span class="k">WARN</span>
</span><span class='line'><span class="nt">&lt;/filter&gt;</span>
</span><span class='line'><span class="nt">&lt;filter&gt;</span>
</span><span class='line'>  <span class="nb">type</span> add_metadata
</span><span class='line'>  <span class="err">include_tag_</span><span class="nb">key</span>
</span><span class='line'>&lt;/filter&gt;
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;match</span> <span class="s">**</span><span class="nt">&gt;</span>
</span><span class='line'>  <span class="nb">type</span> stdout
</span><span class='line'><span class="nt">&lt;/match&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>Fluentdの設定ファイルの流儀にしたがって，上から順番に<code>match</code>にマッチしたところまでのFilterが適用されます．
この例では，まず<code>grep</code>が適用され，その次に<code>add_metadata</code>，そしてこれらの結果のEventStreamが<code>stdout</code>に渡されます．<br />
例えば以下のデータが入ってきたとして:</p>

<pre><code>{"message":"INFO"}
{"message":"WARN"}
</code></pre>

<p><code>stdout</code>には，<code>grep</code>で最初のレコードがフィルタリングされ，<code>add_metadata</code>でタグが追加された以下のデータが渡されます:</p>

<pre><code>{"message":"WARN", "tag":"filter.test"}
</code></pre>

<p>タグを書き換えて<code>match</code>の条件を調整して〜という煩わしさから解放されるようになります．</p>

<p>注意点としては，マッチした<code>match</code>の下にある<code>filter</code>は，たとえtagがマッチしようが適用されない所です．</p>

<h3>パフォーマンス</h3>

<p>タグの書き換えや再emitが発生しないため，速度やメモリ効率は多少よくなるはずです．</p>

<p>手元で約450MBのファイルを<code>in_tail</code>で読み込み，<code>record_reformer</code>でホスト名などを付加して転送する，
というのをv0.10とv0.12で測ってみた所，v0.10は181秒，v0.12は149秒で，約30秒短縮されました．</p>

<p>ユーザによってはもっとFilterがチェーンしている所もあるはずなので，そういう場合には恩恵が大きいと思います．</p>

<h2>Label</h2>

<p>これはFilterやOutputをまとめるための機能になります．制限として，InputはLabelの中に含めることは出来ません．
こちらは先に設定から見てみます．</p>

<h3>設定</h3>

<p>以下が簡単な例になります．</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class='aconf'><span class='line'><span class="nt">&lt;source&gt;</span>
</span><span class='line'>  <span class="nb">type</span> forward
</span><span class='line'>  <span class="err">@</span><span class="nb">label</span> @forward
</span><span class='line'><span class="nt">&lt;/source&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;source&gt;</span>
</span><span class='line'>  <span class="nb">type</span> tail
</span><span class='line'>  <span class="nb">path</span> <span class="sx">/path/to/file</span>
</span><span class='line'>  <span class="nb">tag</span> tail
</span><span class='line'><span class="nt">&lt;/source&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;filter</span> <span class="s">**</span><span class="nt">&gt;</span>
</span><span class='line'>  <span class="nb">type</span> grep
</span><span class='line'>  <span class="c"># ...</span>
</span><span class='line'><span class="nt">&lt;/filter&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;match</span> <span class="s">**</span><span class="nt">&gt;</span>
</span><span class='line'>  <span class="nb">type</span> mongo
</span><span class='line'>  <span class="c"># ...</span>
</span><span class='line'><span class="nt">&lt;/match&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;label</span> <span class="s">@forward</span><span class="nt">&gt;</span>
</span><span class='line'>  <span class="nt">&lt;filter</span> <span class="s">**</span><span class="nt">&gt;</span>
</span><span class='line'>    <span class="nb">type</span> add_metadata
</span><span class='line'>    <span class="err">include_tag_</span><span class="nb">key</span>
</span><span class='line'>  &lt;/filter&gt;
</span><span class='line'>  <span class="nt">&lt;match</span> <span class="s">**</span><span class="nt">&gt;</span>
</span><span class='line'>    <span class="nb">type</span> file
</span><span class='line'>  <span class="nt">&lt;/match&gt;</span>
</span><span class='line'><span class="nt">&lt;/label&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p><code>forward</code>の中に<code>@label @forward</code>というパラメータがあります．これがLabelの指定になっていて，
<code>@label</code>があるとその指定されたLabelにイベント群が流れて行きます．なので，この例では<code>forward</code>で
受け付けたイベント群は，<code>add_metadata</code>フィルタを通り，その下の<code>file</code>に到達します．<br />
一方，<code>@label</code>のないInputは今までと同じ挙動となり，トップにイベント群を流します．
この例では，<code>grep</code>フィルタを通り，その下の<code>mongo</code>に到達します．</p>

<p>Labelの機能によって，各プラグイン間の関係がわかりやすくなり，またルーティングのためにタグを調整する必要もなくなります．
<code>out_relabel</code>プラグインを使えばデータを別のLabelに飛ばせるので，入力によって違うフィルタを適用したいが，書き込み先は一緒にしたい，
みたいな処理も簡単にできるようになります．</p>

<h3>実装</h3>

<p>v0.12から，<code>router</code>というのがInput/Outputに増えてます．今まで<code>Engine.emit</code>と書いていた所を，<code>router.emit</code>に書き換えるだけで，
Labelの機能が使えるようになります．<br />
言い換えれば，この書き換えをしていないInput/Outputプラグインは，<code>@label</code>が使えないということになります．</p>

<h2>まとめ</h2>

<p>v0.12の主要機能であるFilterとLabelについて簡単に説明しました．まだ実装としていくつか細かな改善はあるのですが，
基本機能は上記のままで行くと思います．<br />
何かフィードバックがあれば<code>@repeatedly</code>にmentionをするか，上記の該当PRにコメントをして貰えればと思います．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fluentd UI]]></title>
    <link href="http://repeatedly.github.com/ja/2014/08/fluentd-ui/"/>
    <updated>2014-08-04T04:10:00+09:00</updated>
    <id>http://repeatedly.github.com/ja/2014/08/fluentd-ui</id>
    <content type="html"><![CDATA[<p><a href="https://github.com/fluent/fluentd-ui">fluent/fluentd-ui</a></p>

<p><img src="http://repeatedly.github.com/images/fluentd_ui_ss.png" title="&#34;Fluentd UI image&#34;" alt="&#34;Fluentd UI image&#34;"></p>

<p>Fluentdのエコシステムの一つとして，Fluentd UIをリリースしました．
すでに試してくれたユーザもいるようなので，現在の使用感などは下記の記事を参考にしてください．</p>

<ul>
<li><a href="http://suzuken.hatenablog.jp/entry/2014/08/01/202334">Fluentd UIが出たので触ってみた</a></li>
<li><a href="http://qiita.com/inokappa/items/0a4a2db7abad22458bea">Touch the fluentd-ui(1)</a></li>
</ul>


<p>この記事ではFluentd UIそのものについてつらつらと書きたいと思います．英語でのアナウンスもいずれ公式ブログに載るはず．</p>

<h2>Fluentd UIの生い立ち</h2>

<p>Fluentd UIの背景として，Fluentdも最近は国を問わず色々な所でユーザが増えてきており，
「CLIとか楽勝！」以外のユーザの割合も増えつつあります．</p>

<p>ログコレクタでリッチな管理UIを持っているプロダクトってほとんどないと思うのですが，
新しく使い始めるユーザの嵌まり所とか見ていると，
GUIの方が始めるための敷居が下がりそうなポイントがいくつかありました．</p>

<p>なので，Fluentdを使いたい人がなるべく嵌まらずに始められるようにと，
万葉の方と開発したのがFluentd UIです．</p>

<h2>Fluentd UIのコンセプト</h2>

<p>生い立ちの所で書いた通り，Fluentdの敷居を下げるのが第一の目的なので，
すでにFluentdをバリバリ使っている人はユーザの対象ではありません．</p>

<p>バージョン0.1.0の段階で，以下の機能があります:</p>

<ul>
<li>Fluentdのプロセスの管理 (start/stop/restart)</li>
<li>Fluentdの設定ファイルの管理と編集</li>
<li>Fluentdのプラグインの管理 (install/uninstall/update)</li>
<li>ログやシステム情報のビューワー，などなど</li>
</ul>


<p>最初に色々機能を洗い出したのですが，全部入れるとリリースが遅れるし，
複雑なのを入れるとユーザが混乱するかもということで，ベーシックな機能セットに絞りました．<br />
それでも，プラグイン入れての挙動チェックなど，全部Webから出来るようになっています．また，
ユーザがテストしにくかったin_tailは，<a href="http://fluentular.herokuapp.com/">Fluentular</a>を参考にかなり使いやすいUIになったと思います．</p>

<p>実装的にもかなり依存を減らしていて，例えば，最初はSQLiteベースだったのですが，
「SQLiteだとFluentd UIそのもののセットアップに嵌まる人が出る可能性がある」とファイルベースに変更したりしました．</p>

<h2>Fluentd UIの今後の展望</h2>

<h3>機能追加</h3>

<p>バージョン0.1.0ということから分かる通り，まだまだ始まったばかりのプロダクトで，
今回は開発者側が「こういう機能があったら始めやすいだろう」というものを中心に入れました．<br />
今後はユーザからのフィードバックも受けつつ，更に使いやすさを改善して行けたらなと．</p>

<p>現状各プラグインの設定を削除する時は，自分でフリーフォームから消す必要があるので，
この辺もボタンで消せるようにとか，出来たらいいかなぁと考えてます．<br />
あと，今設定が組み込みで実装してあるので(インストールした3rd partyプラグイン用の設定ページがない)，
各プラグインから自動で設定ページが生成できるような仕組みも必要かなと，色々と模索中です．</p>

<h3>td-agent 2への同梱</h3>

<p>td-agentには同梱予定はないんですが，td-agent 2にはtd-agent-uiみたいな感じで同梱する予定です．すでに起動した人は知っていると思いますが，td-agentをセットアップする用のボタンがあります :)</p>

<h3>他の環境との連携</h3>

<p>Fluentd UIは，UI上で色々と試行錯誤しその結果出来た設定ファイルをChefとかで配布，
みたいなのを想定しているのですが，毎回コピペとか面倒ですし，
最近だと<a href="https://github.com/sonots/fluentd-server">fluentd-server</a>とかもあるので，この辺なんか上手くやりたいな，と．</p>

<h2>まとめ</h2>

<p>Fluentd UIについて背景含めつらつらと書きました．是非試して頂いて，問題があったら<a href="https://github.com/fluent/fluentd-ui/issues">issue</a>投げるなり<a href="https://github.com/fluent/fluentd-ui/pulls">PR</a>なりしてもらえると助かります！</p>

<p>あ，ちなみに最初のリリースは俺がやりましたが，開発はコミットを見れば分かる通り<a href="https://github.com/uu59">uu59</a>さんでちゃんとしたRailsコードになっているので，ご心配なく！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fluentdとログ収集のパターン]]></title>
    <link href="http://repeatedly.github.com/ja/2014/07/fluentd-and-log-forwarding-patterns/"/>
    <updated>2014-07-31T18:35:00+09:00</updated>
    <id>http://repeatedly.github.com/ja/2014/07/fluentd-and-log-forwarding-patterns</id>
    <content type="html"><![CDATA[<p>「ログを集めて保存する」と言うのは簡単だけど，ログ収集の構成にはいくつか方法があり，勉強会などでちょくちょく聞かれるので，いくつかのパターンについて書く．<br />
「俺はもうバリバリログ収集やってるぜ！」という人は多分すでに知っていることが書かれているので，タブを閉じて良い．</p>

<p>ここではログコレクタにFluentdを想定しているが，他のログ収集プロダクトにも適用出来るはず．
ただ，Fluentdはタグベースのルーティングを持ち，単体でもキューのように動作させることが可能で，既存のものより複雑な問題を解決しようとしているので，少し工夫が必要かもしれない．<br />
Fluentdそのものについては<a href="http://docs.fluentd.org/articles/quickstart">公式ドキュメント</a>や，<a href="http://tagomoris.hatenablog.com/entry/2013/12/03/150656">Fluentdとはどのようなソフトウェアなのか</a>を参考に．</p>

<h2>クライアントから直接保存する</h2>

<p><img src="http://repeatedly.github.com/images/fluentd_only_client_pattern.png" title="&#34;Only client pattern&#34;" alt="&#34;Only client pattern&#34;"></p>

<p>いきなりFluentdを使わないパターン．JavaScript SDKを提供している解析サービスやモバイル端末などでよく使われている．ログがユーザサイドで発生する場合にはログコレクタを仕込むことは出来ないので，基本このアプローチになる．<br />
これらはクライアントの出来によって精度が左右されるが，モバイルであれば繋がらなかったら一定量バッファリングをし，ネットワークが繋がった段階で非同期に転送などがよくある実装．</p>

<p>ログコレクタを用意出来るサーバサイドでこのアプローチを取っているシステムは最近だとあまり聞かない．もしやるならエラーハンドリングとして</p>

<ul>
<li>バッファリング</li>
<li>リトライ</li>
<li>ロードバランシング</li>
</ul>


<p>などの問題を解決する必要がある．Fluentdであればこれらの機能が最初からついているので，今から構築するシステムでやる必要はほとんど無いと思われる．</p>

<h2>末端ノードのログコレクタから直接保存する</h2>

<p><img src="http://repeatedly.github.com/images/fluentd_only_leaf_node_pattern.png" title="&#34;Only leaf node pattern&#34;" alt="&#34;Only leaf node pattern&#34;"></p>

<p>アプリケーションやノードからログを集め，ログコレクタから直接保存する．「クライアントから直接保存する」の例で，ローカルにログコレクタを置き，そこ経由でデータを投げつけるモデル．<br />
ネットワーク周りのややこしい問題をログコレクタに任せられ，Fluentdであればプラグインで容易にストリームを操作できるのがメリット．また，ApacheやNginxから直接外部にデータを投げるのは難しいので，Fluentdでファイルにはき出されたログを収集するときにはこの構成となる．</p>

<p>この構成だと，収集対象のサーバが多くなるだけアクセスが増えて保存先への負荷があがるので</p>

<ul>
<li>データのストリームがそれほど大きくない</li>
<li>保存先が書き込みに対して耐性がある (クラウドサービスなど)</li>
</ul>


<p>などの場合に有効になる．次で書く集約ノードが必要ないので，その分管理は楽になる．<br />
Fluentdは転送役と集約役に違いはないので，この段階でフィルター系プラグインを使って，データを加工することもある．</p>

<p><a href="http://www.treasuredata.com/">Treasure Data</a>の場合には，<a href="https://metrics.librato.com/">Librato Metrics</a>に投げる所などは集約ノードを経由せず，各Fluentd(Treasure Agent)から直接投げている．この場合は各ノードのメトリックスが取りたいので，集約ノードでデータストリームをまとめる必要もない．</p>

<p>これとは違いMongoDBの場合だと，細かいのをちまちま送るより大きめのバッチでガンと書き込んだ方が効率が良いので，このモデルよりも集約ノードを置いた方が良い．</p>

<h2>集約ノードのログコレクタを経由して保存する</h2>

<p><img src="http://repeatedly.github.com/images/fluentd_aggregation_node_pattern.png" title="&#34;Using aggregation node pattern&#34;" alt="&#34;Using aggregation node pattern&#34;"></p>

<p>ログコレクタでの多段構成モデル．集約ノードを置く利点は，データストリームをまとめてデータに対する処理をしやすくする点が大きい．また，各ノードから集めることでストリームが太くなり，マイクロバッチの効率も良くなる．</p>

<p>Fluentdでもアグリゲーションをしたり，<a href="http://norikra.github.io/">Norikra</a>と連携するプラグインがあったりするので，その手の処理はこの集約ノードでやるのがベター．</p>

<p>大抵のログコレクタと同じく，Fluentdのデフォルトの転送プロトコルはPush型になっていて，多段構成時には流れるように保存先にデータが集まるようになっている．<br />
多段構成時の注意点は，ある場所のノードが落ちた時にストリームが途切れてしまう所．Fluentdの場合は障害に対応するために，転送先を複数指定可能で，その中で負荷分散とノードが落ちた時の処理を指定出来るようになっていて，よほど一気にノードが落ちない限りストリームが途切れることはない．もし落ちてもバッファリングが行われるので，ノードを再度立ち上げればデータはまた流れ出す．</p>

<h3>PaaS上でのアプリケーション</h3>

<p>アプリケーションをホストするような環境だとローカルにFluentdを置けなかったりするので(HerokuだとFluentdは起動できるが，ファイルバッファが使えなかったりする．ユースケース次第)，その場合はいきなり集約ノードやキューにデータを投げることになる．</p>

<h2>キューを置く</h2>

<p><img src="http://repeatedly.github.com/images/fluentd_queue_pattern.png" title="&#34;Using queue pattern&#34;" alt="&#34;Using queue pattern&#34;"></p>

<p>データストリームの中でキューを間に挟むことの主な理由は，ログのreliabilityの向上と，処理の間にワンクッション置くこと．</p>

<p>そもそもキューはPush型ではなくPull型であり，Producer・Queue・Consumerと登場人物も増えるので，キューを置くことでログ収集そのものが効率化されることはあまりない．異なる粒度のシステム間で連携をする時に，間にキューを置くことが多い．</p>

<p>実際Fluentdとキューを連携させている例はいくつかあり，たとえば複雑なシステムを構築している<a href="http://engineering.viki.com/blog/2014/data-warehouse-and-analytics-infrastructure-at-viki/">Vikiの例</a>ではKestrelでFluentdからのデータストリームを分岐させているし，AWSは本家が<a href="https://github.com/awslabs/aws-fluent-plugin-kinesis">fluent-plugin-kinesis</a>を書いているので，いずれこの事例も増えるはず.</p>

<p>KafkaやKinesisなど信頼性のあるキューを間に置けば，ログをそれなりの期間保持しつつ，Consumerを複数用意することでストリームを分岐できる．<br />
Push型での分岐とは違い，キューからログを取る側の都合でデータを処理出来るので，各データストリームで処理粒度が違う時に有効．
ただ，キューそのものに信頼性があってもProducer/Consumerが駄目だと効率が良くならずログが欠損/重複するので，自分たちのシステム要件に合わせてちゃんと構築する必要がある．</p>

<h3>Pull型のログ収集</h3>

<p>キューを置かずにPull型でログを転送する方法もある．Kafkaほどの信頼性を期待するのは難しいが，Pull型の利点を享受できる．まだ完成していないが，Fluentdだとモリス=タゴという人が，<a href="https://github.com/tagomoris/fluent-plugin-pullforward">pullforward</a>というのを開発中らしい．</p>

<h2>Fluentdを使わない方が良いパターン</h2>

<p>プロダクションで使われているパターンをいくつか書いた．大抵のシステムは上のどれかの構成に当てはまる．FluentdはWebや広告のみならずIoTやPOS含め色々なところで使われていて，データのアーカイブ，簡単なストリーム処理，ノードやアプリケーションのモニタリング，などなど様々なユースケースの基盤になれる柔軟性がある．</p>

<p>また，LINE社で2年間問題なく動いているし，最近勉強会とかで話を聞くと「安定していて自分の仕事がない」と言われる位，よほど変なことしない限り安定性もある．ある会社で100億events/dayをFluentdクラスタで転送した実績もあるので，パフォーマンスもそれなりにある(フィルタ的な処理を重ねまくると，もちろん遅くなる)．</p>

<p>が，Fluentdを使わない方が良いケースもある．以下主に二つ．</p>

<h3>Exactly Onceが必要なケース</h3>

<p>一切ログの欠損も重複も許せない，しかも確実に書き込む必要がある，というケース．<br />
ストリーム処理でExactly Onceを保証するのはかなり難しく，俺が知っている限りOSSでは存在していない．商用でもほとんど知らない．</p>

<p>Exactly Onceを実現しようとすると，ログ発生時点から書き込むまでを完全に同期的にやるか，様々な箇所でトランザクションが必要になり，スケールアウトが難しくコストもものすごく掛かる．そして保存先にもそれなりの機能が求められる．</p>

<p>かつてOSSでマスターノードを使い到達保証を目指したプロダクトがあったが，結局パフォーマンスがボトルネックになり，そこまでしてもログの欠損が防げなかったため，新しいバージョンでは諦めることになった．</p>

<h4>Fluentdのモデル</h4>

<p>FluentdはAt Most Onceであり，<a href="http://docs.fluentd.org/articles/high-availability">ドキュメントにもモデルの説明と対策が書いてある</a>，<br />
これはログが欠損/重複しまくるということではなく，At Most OnceもAt Least Onceも通常は問題は起きない．トポロジーに問題がある場合に，システム的に欠損/重複が起きうるという話．そして大抵のユーザはExactly Onceでなくても上手く行く．</p>

<p>Fluentdでも欠損/重複を防ぐための対策はいくつかあり</p>

<ul>
<li>ログにユニークIDを足して，HadoopなどのETLで重複を削除する</li>
<li>直近の生ログは保持しておき，保存先とのレコード数を比較する．おかしい所は再送</li>
<li>データストリームを複数作って冗長性を持たせる</li>
</ul>


<p>などなど．システム的にExactly Onceではなくても，それに低コストで近づけることは可能．</p>

<h3>データストリームが一本で密結合したサービスがある</h3>

<p>たとえばAWSのCloudWatch Logsがそう．Agentのセットアップが必要だったりして楽かと言われるとまだ微妙だが，今後最初からインストールされる可能性もある．<br />
そのような状況で，Agentから取れるログをCloudWatch Logsで直接見るので十分であれば，Fluentdを入れるメリットは少ない(<a href="https://github.com/ryotarai/fluent-plugin-cloudwatch-logs">CloudWatch Logs</a>用のプラグインがあるので，Fluentdから利用することは可能)．</p>

<p>Fluentdの利点は簡単にロバストで柔軟性のあるログ収集基盤が作れて，一度構築してしまえば，環境非依存でそのログ収集基盤を再利用出来るところなので，そのような必要性がないのであれば，環境と密結合したサービスを使う方が運用コストそのものは直近は削減できるはず．</p>

<h2>まとめ</h2>

<p>Fluentdが向いているケースと向いてないケース，そのPros/Consみたいなのを書いた．細かく書けばもっと色々と書けるのだけど，記事でそんなに長々と書いても疲れるだけなので大まかに．
何か質問があればTwitterで<a href="https://twitter.com/repeatedly">@repeatedly</a>にmentionするなり，この記事にコメントするなりしてください :)</p>

<p>もうすぐFluentd + Elasticsearch + Kibanaという有名な構成のムック本が出るようなので，この本とか読むと，解析のノウハウ含め色々とここには書いていない情報が手に入るのではないかと思います．<br />
<a href="http://www.amazon.co.jp/exec/obidos/ASIN/4774169838/repeatedly-22/ref=nosim/"><img src="http://ecx.images-amazon.com/images/I/61vwvFp6EEL._SL160_.jpg" /></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MPP on Hadoop, Redshift, BigQuery]]></title>
    <link href="http://repeatedly.github.com/ja/2014/07/mpp-on-hadoop-redshift-bigquery/"/>
    <updated>2014-07-23T22:33:00+09:00</updated>
    <id>http://repeatedly.github.com/ja/2014/07/mpp-on-hadoop-redshift-bigquery</id>
    <content type="html"><![CDATA[<p>Twitterで「早く今流行のMPPの大まかな使い方の違い書けよ！」というプレッシャーが半端ないのでてきとうに書きます．この記事は俺の経験と勉強会などでユーザから聞いた話をもとに書いているので，すべてが俺の経験ではありません(特にBigQuery)．各社のSAの人とかに聞けば，もっと良いアプローチとか詳細を教えてくれるかもしれません．<br />
オンプレミスの商用MPPは使ったことないのでノーコメントです．</p>

<p>MPP on HadoopでPrestoがメインなのは今一番使っているからで，Impalaなど他のMPP on Hadoop的なものも似たような感じかなと思っています．
もちろん実装の違いなどがあるので，その辺は適宜自分で補間してください．</p>

<h2>前提</h2>

<p>アプリケーションを開発していて，そのための解析基盤を一から作る．</p>

<h2>簡単なまとめ</h2>

<ul>
<li>データを貯める所が作れるのであれば，そこに直接クエリを投げられるPrestoなどのMPPクエリエンジンを立てるのが楽

<ul>
<li>特にHadoopとかすでにある場合には，まずこれ</li>
</ul>
</li>
<li>データマートが必要であれば，RedshiftやBigQueryの方が向いている</li>
<li>ストレージもスキーマも運用も何も考えずにやりたいのであれば，BigQueryにFluentdとかでとりあえずJSONでぶち込むのが，多分この候補の中では一番初期コストが掛からない

<ul>
<li>もちろん要求によるので気をつける</li>
</ul>
</li>
<li>Presto + Redshiftというケースも有り(アドホックなクエリはPresto，BIからガリガリつなぐのにRedshiftなど)</li>
</ul>


<p>以下つらつらとリスト形式で書いてます．</p>

<h2>Presto/Impala/Drill</h2>

<p>PrestoやImpalaやApache Drillは，Redshift/BigQueryと違ってMPPデータベースではなくてMPPクエリエンジンなので，そこに違いがある．
Prestoそのものについては，@frsyuki が<a href="http://treasure-data.hateblo.jp/entry/2014/07/10/150250">HCJ 2014で発表したスライド</a>があるので，そっちも参考に．</p>

<ul>
<li>PrestoやImpalaはそもそもサービスじゃないので，自分ですべて運用する必要がある．

<ul>
<li>死活監視とか，もう少しワーカーが欲しかったら追加でデプロイみたいなのをする人が必要</li>
</ul>
</li>
<li>外部にストレージが必要

<ul>
<li>すでにある場合には，RedshiftやBigQueryじゃなくて，まずこれで十分かどうか試すという流れになりつつある</li>
<li>PrestoならS3とかにもクエリが投げられるので，FluentdでS3にJSONで貯めて，とかで構築コストはかなり軽減出来る</li>
</ul>
</li>
<li>ストレージ側にスキーマを要求せず，直接クエリが投げられる</li>
<li>MapReduceとかとデータソースを共有出来る

<ul>
<li>データ量が増えてくると，ストレージからMPPデータベースにimportするだけでコストになるので，アドホックにクエリが投げにくくなる</li>
<li>PrestoやApache Drillは一つのクエリで複数のデータソースにアクセス出来るので．マスターテーブルとのjoinとかが楽に出来る</li>
</ul>
</li>
<li>速さに関しては，やはりスキーマありでチューニングされたMPPにはまだ基本勝てない．状況によっては勝てる時もある

<ul>
<li>それでも数秒 - 数十秒で返ってきたりするので，どこまでパフォーマンスを求めるかによる</li>
</ul>
</li>
<li>クエリ同時実行制御に関しては，まだ商用のものに迫っているのはない印象

<ul>
<li>ここの改善に取り組んでいる人たちがいるので，いずれデータマート的にも使えるようになる可能性は高い</li>
</ul>
</li>
<li>オープンソースなので，問題があったら自分で修正可能，処理傾向も把握しやすい</li>
</ul>


<h2>Redshift</h2>

<ul>
<li>スキーマが必須なので，アドホックに解析するためのデータベースとしては少し不向き

<ul>
<li>やるならデータをJSON文字列で保存，クエリ時に分解する．効率は落ちる</li>
</ul>
</li>
<li>MPPデータベース(ParAccel)がAWSカスタマイズされて載っているので，その辺の知識がないと嵌まりやすい

<ul>
<li>AWS Casualで青木さんが話した<a href="http://www.slideshare.net/mineroaoki/20140419-aws-casualredshiftpublic">ふつうのRedshiftパフォーマンスチューニング</a>が参考になる</li>
<li>データ量がすくないとそれでもパワーでなんとかなる</li>
</ul>
</li>
<li>マネージドサービスとはいえ，リーダーノードの挙動がおかしい時など，面倒を見ないと行けないケースはある</li>
<li>カラムを弄る場合は基本テーブルを作り直すのがベター．クラスタ再配置中はREAD ONLYになるので，その辺含めて運用を考える必要がある

<ul>
<li>数時間READ ONLYの場合，貯めようとしていたログはどうするか，など</li>
<li>今時はMPPデータベース単体で運用している所は少ない．RedshiftでもEMRなどからデータを整形して一気にガツンと入れるのが多い</li>
</ul>
</li>
<li>クエリの同時実行制御周りはそれなりに頑張っている

<ul>
<li>データマートして使いやすい．<a href="https://twitter.com/mineroaoki/status/491995296671363073">バッチを投げると厳しい</a>とのこと．</li>
</ul>
</li>
<li>ストレージと演算パワーが分離してないので，ユースケースによってはコストが割に合わない

<ul>
<li>特に周りだとSSDインスタンスを使っている人は<del>いない</del>ほとんどいない(<a href="https://twitter.com/repeatedly/status/489635755979837441">この辺の話</a>)．</li>
<li>某ふっふはっほ社が「たべみる」でSSDをつかってました(<a href="http://www.slideshare.net/mineroaoki/at-aws-summit-tokyo-2014/9">AWS Summitでのスライド</a>)．が，これはかなり<a href="https://twitter.com/mirakui/status/491947294019690497">特殊な</a><a href="https://twitter.com/mirakui/status/491948100571758592">ケース</a>な気も．<a href="https://twitter.com/mineroaoki/status/49">横に並べることによるメリット</a>が大きいようです．</li>
</ul>
</li>
</ul>


<h2>BigQuery</h2>

<ul>
<li>Redshiftと同じく，アドホックに色々とデータをぶち込んで解析するには，JSON文字列(record型？)で保存する必要がある</li>
<li>既存のMPPデータベースとかで考えないと行けない問題(distkeyとか)を，Googleパワーで強引に解決</li>
<li>運用は完全Google任せ

<ul>
<li>Redshiftと違いGoogleの環境を共有するので，細かなことは出来ない</li>
</ul>
</li>
<li>Googleの制約を受ける

<ul>
<li>負荷が極端に上がるようなクエリとかは望み薄(FULL OUTER JOIN, 数億同士のjoin，count(distinct))</li>
<li>Redshift同様，でかいバッチ向けにHDFSとかオブジェクトストレージに生のデータを置いておくのがベター</li>
</ul>
</li>
<li>BigQueryもスキーマがあり，alter column的なものがないので，変更する時は適宜テーブルを作り直すのがベター</li>
<li>Redshiftと違いクエリ単位の課金なので，どれだけ掛かるか把握するのが難しい

<ul>
<li>小さい会社だとかなりマネージしやすい(<a href="https://twitter.com/repeatedly/status/491889139579506688">この辺の話</a>)．大きい会社や，マーケや営業の人がガンガン使う場合には，かなりバーストする可能性がある．Reservedがあるが，<a href="https://developers.google.com/bigquery/pricing?hl=ja#reserved_cap">月200万から</a>なので，判断が難しい</li>
</ul>
</li>
<li>Streaming Importが結構パフォーマンスが出るので，importして即クエリがやりやすい(<a href="http://qiita.com/kazunori279/items/10ac0066ac9b0b5aaaf3">参考記事</a>)</li>
<li>同時実行制御周りは基本よく動く

<ul>
<li>リソースはGoogle側が管理しているので，バッチ投げたら詰まるとかは起きにくい</li>
<li>リソースを占有しているわけではないので，勝手にクエリが死んだり，いきなり遅くなったりすることもある

<ul>
<li>日本時間の21時当たりで起きやすいらしい．でかいバッチを投げまくっているユーザが海外にいる？</li>
</ul>
</li>
</ul>
</li>
</ul>


<h2>まとめ</h2>

<p>基本的にこれらの裏にはHadoopなどの基盤が前提になりつつある(オブジェクトストレージ + 演算リソースでも可)．上のリストを眺めれば，なんとなく使い分けが分かるんじゃないでしょうか！
MPPと一纏めにするにはユースケースや得意なものが違うので，ちゃんと使い分けましょう．基本的にどれもBIツールと接続する方法があり，苦労はそんなにしないはず．</p>

<p>各プロダクトは常に改善されているので，ここに書いてある制約などは，いずれ緩和される可能性があります．常に最新情報をチェックしましょう．</p>

<p>今回はMPPクエリエンジンプロダクトの話なので上のリストでは省きましたが，Treasure Dataが提供しているTQAというMPPサービスは「Prestoで運用とかストレージを考えなくていい版」に近い感じになります．TDはさまざまな<a href="http://docs.treasuredata.com/categories/result">外部ストレージと連携出来る</a>ので，<a href="http://treasure-data.hateblo.jp/entry/2014/05/14/104632">Treasure Dataを中心に使い分ける感じになります</a>．<br />
最後に宣伝！</p>

<h2>P.S</h2>

<p>Twitterに流したさらに要約したtweet.</p>

<blockquote class="twitter-tweet" lang="en"><p>Presto on Storage/BigQuery/Redshiftはユースケースがバラバラなので，適宜使い分ければ良いという単純な話．ただ，HDFSやオブジェクトストレージみたいなのにデータが永続化されている前提なので，Prestoは入りには楽だよね，というのはあります．</p>&mdash; Mr. Fiber (@repeatedly) <a href="https://twitter.com/repeatedly/statuses/491883277829951488">July 23, 2014</a></blockquote>


<script async src="http://repeatedly.github.com//platform.twitter.com/widgets.js" charset="utf-8"></script>




<blockquote class="twitter-tweet" lang="en"><p>Redshiftは同時実行制御頑張っているし，distkeyとかをちゃんと設計してあげればかなりパフォーマンスが出るので，データマートとして優秀．特に8XLとかであれば，RDSとかでは頑張れない領域になる．ただ，スキーマの変更に弱いので，適宜ロードしてあげないと行けないのがネック</p>&mdash; Mr. Fiber (@repeatedly) <a href="https://twitter.com/repeatedly/statuses/491884516605038592">July 23, 2014</a></blockquote>


<script async src="http://repeatedly.github.com//platform.twitter.com/widgets.js" charset="utf-8"></script>




<blockquote class="twitter-tweet" lang="en"><p>BigQueryは逆に，その辺のMPPの知識ないけど楽に処理したい，という時には便利．ストレージと密結合しているRedshiftと違ってストレージとしては安価なので，ガンガンデータを入れられる．スキーマの変更に弱いのと，G社の都合でクエリに制限が掛かったり不安定になるのがネック</p>&mdash; Mr. Fiber (@repeatedly) <a href="https://twitter.com/repeatedly/statuses/491884911515533312">July 23, 2014</a></blockquote>


<script async src="http://repeatedly.github.com//platform.twitter.com/widgets.js" charset="utf-8"></script>




<blockquote class="twitter-tweet" lang="en"><p>Prestoは単体だとストレージがないので，裏側に何を使うかによってかなり左右される．ちゃんとしたHadoopクラスタやオブジェクトストレージがあれば，同時実行制御の部分に難はあるが，低レイテンシクエリーとしては十分優秀．データのインポートの手間がないのが大きい．</p>&mdash; Mr. Fiber (@repeatedly) <a href="https://twitter.com/repeatedly/statuses/491885404665049089">July 23, 2014</a></blockquote>


<script async src="http://repeatedly.github.com//platform.twitter.com/widgets.js" charset="utf-8"></script>




<blockquote class="twitter-tweet" lang="en"><p>まぁHadoopとかあるならとりあえずPresto入れとくか，という流れにはなりつつある気がする．それでも同時にたくさんのクエリが飛んでくるとか，ある程度スキーマ固まったのでガッツリ使いたい，になるとRedshiftとかBigQueryにバッチでinsert，がよくあるパターン</p>&mdash; Mr. Fiber (@repeatedly) <a href="https://twitter.com/repeatedly/statuses/491886467266789376">July 23, 2014</a></blockquote>


<script async src="http://repeatedly.github.com//platform.twitter.com/widgets.js" charset="utf-8"></script>




<blockquote class="twitter-tweet" lang="en"><p>やっぱり他のMPPデータベースにデータ入れるの面倒だからPrestoでがんばりたい，というユーザ結構いてその人たちが色々とPrestoをデータマート的に使えるようにと頑張っているようなので，いずれHadoop上ですべて完結する世界は来るかもしれない</p>&mdash; Mr. Fiber (@repeatedly) <a href="https://twitter.com/repeatedly/statuses/491887539267960833">July 23, 2014</a></blockquote>


<script async src="http://repeatedly.github.com//platform.twitter.com/widgets.js" charset="utf-8"></script>




<blockquote class="twitter-tweet" lang="en"><p>トレジャーデータというサービス，自分でインフラ管理する必要ないですし，Fluentd/JS/iOS/Androidなどからデータを入れるだけで，即バッチや低レイテンシクエリを投げられる，便利なサービスらしいですよ &gt; <a href="http://t.co/gSg9ZTNjKt">http://t.co/gSg9ZTNjKt</a></p>&mdash; Mr. Fiber (@repeatedly) <a href="https://twitter.com/repeatedly/statuses/491889631437144064">July 23, 2014</a></blockquote>


<script async src="http://repeatedly.github.com//platform.twitter.com/widgets.js" charset="utf-8"></script>




<blockquote class="twitter-tweet" lang="en"><p>社会人の勤めを果たした</p>&mdash; Mr. Fiber (@repeatedly) <a href="https://twitter.com/repeatedly/statuses/491889663787819008">July 23, 2014</a></blockquote>


<script async src="http://repeatedly.github.com//platform.twitter.com/widgets.js" charset="utf-8"></script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Release fluent-plugin-http-puma]]></title>
    <link href="http://repeatedly.github.com/ja/2014/07/release-fluent-plugin-http-puma/"/>
    <updated>2014-07-20T18:42:00+09:00</updated>
    <id>http://repeatedly.github.com/ja/2014/07/release-fluent-plugin-http-puma</id>
    <content type="html"><![CDATA[<p><a href="https://github.com/repeatedly/fluent-plugin-http-puma">fluent-plugin-http-puma</a></p>

<p>標準で入っているin_httpとは違って，Pumaと呼ばれるWebサーバベースのHTTP(S)でのリクエストを受け付けるInputプラグイン書きました．</p>

<h2>使い方</h2>

<p>fluent-gemでインストール．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ fluent-gem install fluent-plugin-http-puma</span></code></pre></td></tr></table></div></figure>


<p>in_httpとほぼ同じように動きますが，独自でHTTPリクエストをパースしてないので，keepaliveやボディサイズチェック用のオプションはありません．その代わりPuma関係のオプションが増えてます(<a href="https://github.com/repeatedly/fluent-plugin-http-puma#configuration">README参照</a>)．<br />
一番の違いは，HTTPSをサポートしている所です．<code>use_ssl</code>と<code>ssl_keys</code>を使うことで，HTTPSとして立ち上がります．</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='aconf'><span class='line'><span class="nt">&lt;source&gt;</span>
</span><span class='line'>  <span class="nb">type</span> http_puma
</span><span class='line'>
</span><span class='line'>  <span class="err">use_</span><span class="nb">ssl</span>
</span><span class='line'>  ssl_keys [<span class="s2">&quot;/path/to/key&quot;</span>, <span class="s2">&quot;/path/to/cert&quot;</span>]
</span><span class='line'><span class="nt">&lt;/source&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<h2>パフォーマンス</h2>

<p>手元のMBPで試して見たら，HTTPはin_httpより少し速かった．HTTPSは当たり前ですがｶﾞｸｯと落ちます．</p>

<p>クライアントはRubyの<code>net/http</code>を使って，小さめのjsonを<code>application/json</code>で送ってます．</p>

<ul>
<li>in_http</li>
</ul>


<p>平均2400 events/secくらい．</p>

<pre><code>2014-07-20 19:02:30 +0900 [info]: plugin:out_flowcounter_simple count:2318      indicator:num   unit:second
2014-07-20 19:02:31 +0900 [info]: plugin:out_flowcounter_simple count:2420      indicator:num   unit:second
2014-07-20 19:02:32 +0900 [info]: plugin:out_flowcounter_simple count:2383      indicator:num   unit:second
2014-07-20 19:02:33 +0900 [info]: plugin:out_flowcounter_simple count:2399      indicator:num   unit:second
2014-07-20 19:02:34 +0900 [info]: plugin:out_flowcounter_simple count:2382      indicator:num   unit:second
</code></pre>

<ul>
<li>in_http_puma</li>
</ul>


<p>平均2500 events/secくらい．Ojを使ったらもっと速くなるかと思ったけど，ほぼ誤差の範囲だった．</p>

<pre><code>2014-07-20 19:01:12 +0900 [info]: plugin:out_flowcounter_simple count:2472      indicator:num   unit:second
2014-07-20 19:01:13 +0900 [info]: plugin:out_flowcounter_simple count:2550      indicator:num   unit:second
2014-07-20 19:01:14 +0900 [info]: plugin:out_flowcounter_simple count:2294      indicator:num   unit:second
2014-07-20 19:01:15 +0900 [info]: plugin:out_flowcounter_simple count:2537      indicator:num   unit:second
2014-07-20 19:01:16 +0900 [info]: plugin:out_flowcounter_simple count:2538      indicator:num   unit:second
</code></pre>

<ul>
<li>in_http_puma with VERIFY_NONE client</li>
</ul>


<p>平均400 events/secくらい．けどVERIFY_NONEはほとんど本番では使われないので参考程度．</p>

<pre><code>2014-07-20 19:04:06 +0900 [info]: plugin:out_flowcounter_simple count:406       indicator:num   unit:second
2014-07-20 19:04:07 +0900 [info]: plugin:out_flowcounter_simple count:365       indicator:num   unit:second
2014-07-20 19:04:08 +0900 [info]: plugin:out_flowcounter_simple count:400       indicator:num   unit:second
2014-07-20 19:04:09 +0900 [info]: plugin:out_flowcounter_simple count:399       indicator:num   unit:second
2014-07-20 19:04:10 +0900 [info]: plugin:out_flowcounter_simple count:400       indicator:num   unit:second
</code></pre>

<ul>
<li>in_http_puma with VERIFY_PEER client</li>
</ul>


<p>平均320 events/secくらい．高負荷環境で無ければ大丈夫かな…？</p>

<pre><code>2014-07-20 19:05:18 +0900 [info]: plugin:out_flowcounter_simple count:329       indicator:num   unit:second
2014-07-20 19:05:19 +0900 [info]: plugin:out_flowcounter_simple count:327       indicator:num   unit:second
2014-07-20 19:05:20 +0900 [info]: plugin:out_flowcounter_simple count:327       indicator:num   unit:second
2014-07-20 19:05:21 +0900 [info]: plugin:out_flowcounter_simple count:325       indicator:num   unit:second
2014-07-20 19:05:22 +0900 [info]: plugin:out_flowcounter_simple count:326       indicator:num   unit:second
</code></pre>

<h2>まとめ</h2>

<p>モダンと言われるPumaってどんなもんなんだろう？と調べてたら，HTTPSを標準でサポートしてるし，ライブラリとしても簡単に使えそうだったので試して見たら，それなりにパフォーマンスが出たので公開してみたという感じです．<br />
Pumaなので，今後FluentdがJRubyとかサポートしても普通にそのまま使えると思います．</p>

<p>実際HTTPSを処理するならFluentdの前にNginxとかを置いた方が良いのだけど，バックエンドでも通信はHTTPSでやってるとかなんか縛りがあるような環境では，手軽に使えるのではないかと思います．</p>

<p>何かあれば<a href="https://github.com/repeatedly/fluent-plugin-http-puma">issue/pull request</a>に投げてもらえれば対応します．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Prestoソースコードリーディング #4]]></title>
    <link href="http://repeatedly.github.com/ja/2014/07/presto-scr-4/"/>
    <updated>2014-07-17T17:53:00+09:00</updated>
    <id>http://repeatedly.github.com/ja/2014/07/presto-scr-4</id>
    <content type="html"><![CDATA[<p><a href="http://atnd.org/events/53545">Presto ソースコードリーディング #4</a></p>

<p>いつものようにLINEでやりました！</p>

<p>@frsyukiが帰国する前に，という流れで開催決定・募集が1週間前というタイトなスケジュールでしたが，
無茶ぶりにつきあってくれた@ueshinさんに感謝．</p>

<h2>当日の内容</h2>

<p><a href="http://togetter.com/li/694128">togetterのまとめ</a>を見れば，なんとなく大まかな流れは把握できるはず…！</p>

<p>ueshinさんが第二回のashigeruさんの論理計画実行の後を継いで，
物理計画実行周りの話をしてくれました(<a href="https://gist.github.com/ueshin/5e38cfe915ce15f756b1">資料のgist</a>)．
バイトコード生成しての高速化の話とか，Presto以外でも有用な話が出てました．</p>

<p>frsyukiが現在のPrestoの開発体制の話，Treasure Dataでハックしている所の紹介，
CREATE VIEWなどの実装がなぜこうなっているのか(これはfrsyuki案が通ったらしい)，
今後Prestoチームがやろうとしていることなど含め，その場で色々と議論する感じになりました．</p>

<h2>まとめ</h2>

<p>少人数でソースコードレベルで興味のある人が集まっているのもあり，いつも通り濃い感じでした！
partitionedの話とか，現在の実行モデルにおけるオペレータの実装の話とか，
この辺を話すイベントはあんまりないんじゃないかなと思います．</p>

<p>次第5回をいつやるかは決まってませんが，いずれやる予定です．</p>

<p>またソースコードリーディングとは別に，Prestoのユースケースや，
プロダクションで動かしている人の運用の話などを共有する，
Presto meetupみたいなもう少し緩めのイベントもやる予定です．</p>

<p>お楽しみに :)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Release fluent-plugin-multi-format-parser]]></title>
    <link href="http://repeatedly.github.com/ja/2014/07/release-fluent-plugin-multi-format-parser/"/>
    <updated>2014-07-13T21:00:00+09:00</updated>
    <id>http://repeatedly.github.com/ja/2014/07/release-fluent-plugin-multi-format-parser</id>
    <content type="html"><![CDATA[<p><a href="https://github.com/repeatedly/fluent-plugin-multi-format-parser">fluent-plugin-multi-format-parser</a></p>

<p>一つのログファイルの中に複数のフォーマットがある時に利用可能なパーサプラグイン書きました．</p>

<h2>使い方</h2>

<p>fluent-gemでインストール．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ fluent-gem install fluent-plugin-multi-format-parser</span></code></pre></td></tr></table></div></figure>


<p>インストールすると，TextParserを利用している<code>in_tail</code>や<code>in_udp</code>などで<code>multi_format</code>が使えるようになります．
そこで<code>&lt;pattern&gt;</code>を複数並べてください．<code>&lt;pattern&gt;</code>内の各設定は，利用するパーサにそのまま渡されます．</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='aconf'><span class='line'><span class="nt">&lt;source&gt;</span>
</span><span class='line'>  <span class="nb">type</span> udp
</span><span class='line'>  <span class="nb">tag</span> logs.multi
</span><span class='line'>
</span><span class='line'>  <span class="nb">format</span> multi_format
</span><span class='line'>  <span class="nt">&lt;pattern&gt;</span>
</span><span class='line'>    <span class="nb">format</span> apache
</span><span class='line'>  <span class="nt">&lt;/pattern&gt;</span>
</span><span class='line'>  <span class="nt">&lt;pattern&gt;</span>
</span><span class='line'>    <span class="nb">format</span> json
</span><span class='line'>  <span class="nt">&lt;/pattern&gt;</span>
</span><span class='line'>  <span class="nt">&lt;pattern&gt;</span>
</span><span class='line'>    <span class="nb">format</span> <span class="k">none</span>
</span><span class='line'>  <span class="nt">&lt;/pattern&gt;</span>
</span><span class='line'><span class="nt">&lt;/match&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p><code>&lt;pattern&gt;</code>を上から順番に試し，パース出来たらその結果を返します．上の設定例だと，最初に<code>apache</code>，次に<code>json</code>，最後に<code>none</code>になります．
たとえば，以下のようなログ群を<code>in_udp</code>に流すと</p>

<pre><code>192.168.0.1 - - [28/Feb/2013:12:00:00 +0900] "GET / HTTP/1.1" 200 777
{"k1":"v", "k2":123456}
foobar
</code></pre>

<p>以下のように処理されます．</p>

<pre><code>2013-02-28 12:00:00 +0900 test.**: {"host":"192.168.0.1","user":"-","method":"GET","path":"/","code":"200","size":"777"}
2014-07-13 20:56:55 +0900 test.**: {"k":"v","k1":123456}
2014-07-13 20:56:55 +0900 test.**: {"message":"foobar"}
</code></pre>

<h2>まとめ</h2>

<p>ログファイルの中に複数のフォーマットが混じるのは望ましいことではないのですが，どうしてもそういう状況が起きるときには，使ってみてください．</p>

<p>あと，実装はかなり単純なので，パーサプラグインのサンプルにでもしてもらえればと思います．</p>

<p>何か要望があれば<a href="https://github.com/repeatedly/fluent-plugin-multi-format-parser">issue/pull request</a>に投げてもらえれば対応します．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Release fluent-plugin-netflow]]></title>
    <link href="http://repeatedly.github.com/ja/2014/07/release-fluent-plugin-netflow/"/>
    <updated>2014-07-11T21:35:00+09:00</updated>
    <id>http://repeatedly.github.com/ja/2014/07/release-fluent-plugin-netflow</id>
    <content type="html"><![CDATA[<p><a href="https://github.com/repeatedly/fluent-plugin-netflow">fluent-plugin-netflow</a></p>

<p>CiscoのNetflowプロトコルを受け取るFluentdプラグインを書きました．
公開してみたら結構反応が良くて，地味にユーザが多そうですね…</p>

<h2>経緯</h2>

<p>Fluentdを調べていたユーザから「ログ収集基盤をFluentdベースにしたいんだけど，Netflowをパースするプラグインないの？」と言われたので，
さくっと作ってみました．<br />
「さくっと」と言ってもコアのパーサー部分はそのユーザが使っていたLogstashからポーティングしたやつなので，それなりにパース出来るはずです．
Pull Requestのおかげもあり，対応templateも少し増えてます．</p>

<h2>使い方</h2>

<p>fluent-gemでインストールするだけです．</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ fluent-gem install fluent-plugin-netflow</span></code></pre></td></tr></table></div></figure>


<p>その後，以下のように設定します．typeとtagは必須です．他のやつは適宜環境に合わせて設定してください．</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='aconf'><span class='line'><span class="nt">&lt;source&gt;</span>
</span><span class='line'>  <span class="nb">type</span> netflow
</span><span class='line'>  <span class="nb">tag</span> netflow.event
</span><span class='line'>
</span><span class='line'>  <span class="c"># optional parameters</span>
</span><span class='line'>  <span class="nb">bind</span> <span class="m">127.0.0.1</span>
</span><span class='line'>  <span class="nb">port</span> <span class="m">9555</span>
</span><span class='line'>
</span><span class='line'>  <span class="c"># optional parser parameters</span>
</span><span class='line'>  <span class="err">cache_</span><span class="nb">ttl</span> <span class="m">6000</span>
</span><span class='line'>  <span class="nb">versions</span> [5, <span class="m">9</span>]
</span><span class='line'>  <span class="nb">definitions</span> <span class="sx">/path/to/your_definitions.yml</span>
</span><span class='line'><span class="nt">&lt;/match&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>今現在はin_syslogの影響もあってデフォルトポートが5140のままなのですが，0.1.0を出す時に変更する予定です．</p>

<h2>TODO</h2>

<ul>
<li>俺自身がNetflow関係のやつを持っていないので，使っている人，誰かメンテナになってください！実環境でのテストが出来ませんｗ</li>
<li>Fluentd v0.10.52が出たら，FluentdのSocketUtilベースで書き直して，0.1.0を出す</li>
</ul>


<p>他，何か要望があれば<a href="https://github.com/repeatedly/fluent-plugin-netflow">issue/pull request</a>をしてもらえれば対応します．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Fluentd v1 and Roadmapというプレゼンをしてきた]]></title>
    <link href="http://repeatedly.github.com/ja/2014/05/fluentd-v1-and-roadmap/"/>
    <updated>2014-05-15T18:55:00+09:00</updated>
    <id>http://repeatedly.github.com/ja/2014/05/fluentd-v1-and-roadmap</id>
    <content type="html"><![CDATA[<p><a href="http://eventdots.jp/event/49560">Fluentd Meetup 新しい応用事例とv1に関する発表</a></p>

<p>フリークアウトさんのオフィスでFluentd Meetupがあり，Fluentdのv1について話してきました．</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/34652297?rel=0" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/treasure-data/fluentd-v1-and-roadmap" title="Fluentd v1 and Roadmap" target="_blank">Fluentd v1 and Roadmap</a> </strong> from <strong><a href="http://www.slideshare.net/treasure-data" target="_blank">Treasure Data, Inc.</a></strong> </div></p>

<p>今回の発表は，今までのv11やv1に関してのまとめ的な発表になっています．<br />
以下のリンク集を見れば，発表内容の大抵はカバー出来ると思います．
また，他の方もまとめ記事とかを書かれているので，そちらも参照してください．</p>

<ul>
<li><a href="http://repeatedly.github.io/ja/2014/03/about-fluentd-v11/">そろそろFluentd v11についてひとこと言っておくか</a></li>
<li><a href="https://github.com/fluent/fluentd/issues/251">Plan for v1 release #251</a></li>
<li><a href="https://github.com/fluent/fluentd/issues/317">Support JRuby #317</a></li>
<li><a href="https://github.com/fluent/fluentd/tree/windows">FluentdのWindowsブランチ</a></li>
<li><a href="https://github.com/fluent/fluentd/pull/293">Add &#8211;use-v1-config option to enable new configuration format #293</a></li>
<li><a href="https://github.com/repeatedly/omnibus-td-agent">td-agent2のパッケージリポジトリ</a></li>
</ul>


<p>俺の方から言えることは，Fluentd v0.10.46以降を使っている方は，
積極的に<code>--use-v1-config</code>オプションを使ってくださいということです．<br />
既存のフォーマットとの違いは<a href="http://docs.fluentd.org/articles/config-file#v1-format">ドキュメントを参照</a>してください．</p>

<p>td-agent2は今ビルド中で，今週末あたりにリリースしたいと思ってます(パッケージングツールをアップデートしたら色々と壊れて無駄に時間が…)．
それ以降はv1にリソースを割けるはずなので，フィルタやラベルなどを実装していく予定です．ご期待ください！</p>

<p>また，Treasure DataはFluentdの開発者含め<a href="http://www.treasuredata.com/jp/about/careers.php">人材を募集している</a>ので，
興味のある方はぜひぜひコンタクトをしてもらえればと思います！</p>

<p>P.S.</p>

<p>フリークアウトさんのオフィスにある，バスケのゴールにダンク出来なかったのが悔しいので，足腰鍛え直す．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MultiOS testing for Fluentd repository on Travis CI]]></title>
    <link href="http://repeatedly.github.com/2014/05/multios-testing-for-fluentd-repository-on-travis-ci/"/>
    <updated>2014-05-15T14:22:00+09:00</updated>
    <id>http://repeatedly.github.com/2014/05/multios-testing-for-fluentd-repository-on-travis-ci</id>
    <content type="html"><![CDATA[<p>Recently, Travis CI announces &#8220;Multi-OS feature&#8221; on their blog.</p>

<p><a href="http://blog.travis-ci.com/2014-05-13-multi-os-feature-available/">Multi-OS Feature Available</a></p>

<p>If this feature is enabled, our project can be tested on both Linux and Mac OS X. <br />
Multi OS feature has been enabled for <a href="https://github.com/fluent/fluentd/commit/e4bb4f61e91c70a95690d19b07414b485e7a3542">Fluentd repository</a>. See following build:</p>

<p><a href="https://travis-ci.org/fluent/fluentd/builds/25147077">fluent/fluentd on Travis CI</a></p>

<p>This is very useful for checking commit and PR. Thanks Travis CI and Facebook team!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[The status of Cool.io]]></title>
    <link href="http://repeatedly.github.com/2014/05/the-status-of-coolio/"/>
    <updated>2014-05-06T02:55:00+09:00</updated>
    <id>http://repeatedly.github.com/2014/05/the-status-of-coolio</id>
    <content type="html"><![CDATA[<p><a href="https://github.com/tarcieri/cool.io">Cool.io</a> was revived after I became a maintainer. This article describes the current stauts and future work of Cool.io.</p>

<p>Cool.io is a core of <a href="http://fluentd.org/">Fluentd</a> so I am focusing Fluentd related features.</p>

<h2>Windows support</h2>

<p>Cool.io is the one of blocker for Fluentd Windows support. Since version 1.2, Cool.io works on Windows and I release mingw based cross-compiling gem for Windows environment.</p>

<p>You can now use Cool.io via <code>gem install cool.io</code> on Windows and we started to implement <a href="https://github.com/fluent/fluentd/tree/windows">Windows support of Fluentd</a>.</p>

<h3>The limitation on Windows</h3>

<p>Cool.io uses <code>select</code> on Windows, not IOCP. In this result, the performance isn&#8217;t better than other environments.
This limitation is of little concern in Fluentd forward use-case.</p>

<h2>JRuby support</h2>

<p>Fluentd works on CRuby and Rubinius. If support JRuby, Fluentd will work on almost popular Ruby environemnts.</p>

<p>Since May 2014, <a href="https://github.com/taichi">@taichi</a> started to implement <a href="https://github.com/tarcieri/cool.io/issues/32">JRuby support</a>. @taichi has experience with Java and evented IO frameworks so he is a good person for this task ;)</p>

<h2>Remove Ruby 1.8 support</h2>

<p>Ruby 1.8 is dead language so Cool.io removes 1.8 support since version 1.2.</p>

<h2>Remove HttpClient</h2>

<p>AFAIK, there is no HttpClient users including Fluentd. This class depends on Ragel to parse HTTP. It increases the maintenance cost and hard to support on cross platform.</p>

<p>So HttpClient will not be maintained in the future.</p>

<h2>Conclusion</h2>

<p>Cool.io isn&#8217;t dead and Cool.io is still improved. We are now working on more better environment support and fixing the bugs.</p>

<p>If you have a bug of Cool.io, then please file it on github :)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[札幌 2014 冬]]></title>
    <link href="http://repeatedly.github.com/ja/2014/03/sapporo-2014-winter/"/>
    <updated>2014-03-22T15:41:00+09:00</updated>
    <id>http://repeatedly.github.com/ja/2014/03/sapporo-2014-winter</id>
    <content type="html"><![CDATA[<p>二つのイベントに参加するため，札幌に行ってました．</p>

<h2><a href="http://www.hicta.or.jp/index_oshirase_detail.php?id=624">技術セミナー２０１４～ビッグデータ時代のＩＴシーズ～</a></h2>

<p>北海道IT推進協会さまのイベントで，ビッグデータとかデータ解析周りについてクラウド要素も含めて，ビジネス的な話をしてきました．
参加者に経営者の方も多い感じだったので，なるべく技術的な用語は使わないよう心がけて講演．</p>

<p>いつもとは違う雰囲気の中での講演で，こういうのも勉強になって良いかなと．</p>

<h2><a href="http://connpass.com/event/5222/">AWS勉強会 in 北海道札幌！</a></h2>

<p>メインは上のイベントだったのだけど，それだけで北海道を去るのも勿体ないなぁと他のイベントを探していたら</p>

<blockquote class="twitter-tweet" data-conversation="none" lang="en"><p>話す側でお願いしますw <a href="https://twitter.com/repeatedly">@repeatedly</a> あれ，よく見たらクラスメソッドさんの勉強会が次の日にある．これに参加しようかな？ &gt; &quot;AWS勉強会 in 北海道札幌！（懇親会有）&quot; <a href="http://t.co/doSVv4FPb8">http://t.co/doSVv4FPb8</a></p>&mdash; よこたさとし (@sato_shi) <a href="https://twitter.com/sato_shi/statuses/441211073375371264">March 5, 2014</a></blockquote>


<script async src="http://repeatedly.github.com//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>とのreplyをもらったので，飛び入りでFluentdについて発表してきました．</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/32700881?rel=0" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/treasure-data/fluentd-and-aws-at-classmethod" title="Fluentd and AWS at classmethod" target="_blank">Fluentd and AWS at classmethod</a> </strong> from <strong><a href="http://www.slideshare.net/treasure-data" target="_blank">Treasure Data, Inc.</a></strong> </div></p>

<p>Fluentd利用者も少なかったので，基本的な動作原理や，Fluentdが有用なユースケース，
AWSなイベントなのでAWS関係のプラグインの紹介などなど，一通りやった．</p>

<p>懇親会で色々と話を聞いたけど，まだ地方？だとそもそもデータを集めてなかったり，
データ解析に関しての需要が出てきておらず，Fluentdなどを使う段階になってないとのこと．
そもそもIT化出来てない会社とかもあるのでまずはそこから，とも<br />
東京とかだとこの辺の動きが活発だけど，中央以外ではまた違った形で活動しないと，
「便利なソフトウェアがある！」とかで広めることは出来ないなぁと実感．<br /></p>

<p>色々と考えないと行けないことが増えた感じで，この辺の話を聞けたのは良かった．</p>

<h2>まとめ</h2>

<p><img src="http://repeatedly.github.com/images/kinki_sushi.jpg" title="&#34;Kinki no Sushi&#34;" alt="&#34;Kinki no Sushi&#34;"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[そろそろFluentd v11についてひとこと言っておくか]]></title>
    <link href="http://repeatedly.github.com/ja/2014/03/about-fluentd-v11/"/>
    <updated>2014-03-05T19:40:00+09:00</updated>
    <id>http://repeatedly.github.com/ja/2014/03/about-fluentd-v11</id>
    <content type="html"><![CDATA[<p>リリースは永遠にされません！</p>

<p>日本では色々なところでv11の噂がまことしやかに囁かれていますが，
俺がメインメンテナである限りv11がリリースされることはないので，諦めてv0.10.xを使ってください！</p>

<p>以下まじめな話になります．</p>

<h2>v11が生まれた背景と現状</h2>

<p>v11が生まれたのは1年以上前です．背景には，v10と呼ばれる今のバージョンがプロトタイプを兼ねたリリースであり，
「利用者のフィードバックを取り込んで，ダメな所をガッツリ書き換えて互換性を壊してメジャーバージョンアップや！」という流れがありました．</p>

<p>しかし，v10は十分に柔軟でかつパフォーマンスも発揮しており，コミッタ陣はそれほどモチベーションがあったわけではありません．
また，プラグインによって解決出来た問題も多く，v11が生まれた時ほどユーザから「v11が欲しい！」という要望は聞かれなくなりました．</p>

<p>当たり前ですが，ユーザからの声がなければ「あれ，じゃあこの機能あんまり必要ないんじゃね？」ということになって削られるのはよくあることで，
v11もそんな感じで開発リソースはあまり割かれなくなって行きました．<br />
もともと空いた時間に@frsyuki，@tagomoris，@sonotsの人達がたまにコミットしていたくらいなので，直近では機能追加のコミットはほぼありません．</p>

<h2>Fluentdのユーザ拡大</h2>

<p>v11で互換性を壊しても大丈夫だろう，とコミッタ陣が思っていたものの一つに，当時Fluentdのプラグインの大半をコミッタの関係者が書いていた，
というのがあります．<br />
そのため，互換性を壊しても必要そうなプラグインの書き換えはすぐに終わるし，
「えいや！」と同時にバージョン上げれば被害は少ないだろうという感じです．</p>

<p>しかし，v10は思っていたよりも早くユーザが増え，今ではさきがけとなったWeb/アドテク業界のみならず，大小問わず様々な企業，
IoTな分野など，世界的にも使われるようになってきました．<br />
また，それにともないプラグイン数も200を越え，当初の目論見が難しくなっているのが現状です．</p>

<h2>開発方針の変更とv1案</h2>

<p>俺がFluentdのメインメンテナとなったのも関わってるんですが，「v11のリリースをどうするか？」という問題もハンドリングするようになりました．</br />
個人的には「v11の機能群は有用だけど，互換性を壊してリリースするほどのものではないなぁ」というのがv11を見た結論であり，
その辺やりとりしたのが以下のtogetter．</p>

<p><a href="http://togetter.com/li/620329">Fluentd v11 なんてなかったんだ</a></p>

<p>上のまとめにあるように，色々と話した結果，v11が持っているいくつかの機能をv10にマージし，
それらが終わったらv1としてリリースしようというのが，現在の開発の方向性です．<br />
段階的な変化になりますが，基本的には互換性が保たれるようにAPIを変えていくので，
既存ユーザはそのまま使えますし，バージョンアップにともなって徐々に有用な機能が使えるようになります．<br /></p>

<p>以下のissueで必要そうな機能リストと，進捗を管理してます．</p>

<p><a href="https://github.com/fluent/fluentd/issues/251">Plan for v1 release</a></p>

<p>v1というバージョンに関しては，今のFluentdのgemのバージョンがv0.10.xで，v0.11.xのことをv11と呼んでいて非常に紛らわしく，
新しく入ってくるユーザとかを考えるとそろそろまじめにバージョニングした方が良いだろうという考えです．<br />
v1だと安定版のイメージが強いですし，実際の所v0.10.xはv1でもおかしくない機能セットはすでに持っています．</p>

<h2>まとめ</h2>

<p>ということで，色々と今までの流れを書きましたが，皆さんFluentdを今後とも宜しくお願いします :)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sensu雑感]]></title>
    <link href="http://repeatedly.github.com/ja/2014/02/the-thoughts-of-sensu/"/>
    <updated>2014-02-25T02:15:00+09:00</updated>
    <id>http://repeatedly.github.com/ja/2014/02/the-thoughts-of-sensu</id>
    <content type="html"><![CDATA[<p><a href="http://sensuapp.org/">Sensu</a></p>

<p>最近人気が出てきているようなので試して見た．
仕組みに関しては本家のドキュメントとかスライドとか見ると大体分かる．</p>

<p>雑感:</p>

<ul>
<li>server, client, api, dashboardに分かれているのは良い

<ul>
<li>実装はRubyでシンプルに書かれているように見える．多分弄るのは簡単</li>
</ul>
</li>
<li>RabbitMQとRedisが必要なのが試すのに結構つらい．chefとかpuppetを使うと良いらしい？

<ul>
<li>なんかテストモードがあるなら知りたい</li>
<li>ドキュメントは最低限はある．Advancedなことしようとするとgithubとか先人を頼ることになる</li>
</ul>
</li>
<li>設定がJSONなのはいいけど，ログすらJSONなのは徹底している</li>
<li>RabbitMQにはクライアントから登録しにいくようで，勝手に監視対象が増えるのは楽

<ul>
<li>マスターからのpullは限界があるので，この仕組みはモニタリングでは筋が良さそう</li>
</ul>
</li>
<li>プラグインは簡単に書けるが，現状失敗したら諦める前提らしい

<ul>
<li>まじめにハンドリングするなら，他のと連携する必要がある</li>
</ul>
</li>
<li>apiが気がつけば落ちてる時があって，原因がよく分からない</li>
<li>Fluentdのハンドラがコミュニティプラグインになかったので<a href="https://github.com/sensu/sensu-community-plugins/blob/master/handlers/notification/fluentd.rb">書いた</a></li>
</ul>


<p>「FluentdかSensuかどっちかに寄せたい」みたいな記事を誰かが書いてた記憶があるけど，何となく分かる．
Fluentdのプラグイン機構でSensuのクライアント周りの機構はよりロバストに実装出来る．<br />
コアの機能は結構シンプルなので，visualizationまで考えるとどうしたもんかという感じ．</p>

<p>Sensuはモニタリングのためのフレームワークなのでその辺の機能が中心に開発されてるけど，
そこの尖った部分が必要でなければ，RabbitMQとかと相まって結構大がかりにも見える．<br />
Nagiosとかは使ったことはないので，その辺と違ってどこまでメリットがあるのかはよく分かってないけど，
新しく作るシステムで一からモニタリングシステムを構築するなら十分有り．</p>

<p>仕組み的には柔軟でAPIとかで管理も楽に出来るようになってるので，今後ユーザは増えそうな気もしている．
しかしRabbitMQとRedisの二つ必要なのどうにかならないものか…</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Developer Summit 2014]]></title>
    <link href="http://repeatedly.github.com/ja/2014/02/developer-summit-2014/"/>
    <updated>2014-02-16T19:53:00+09:00</updated>
    <id>http://repeatedly.github.com/ja/2014/02/developer-summit-2014</id>
    <content type="html"><![CDATA[<p><a href="http://event.shoeisha.jp/devsumi/20140213/">Developers Summit 2014：開発者のためのITカンファレンス</a></p>

<p>セッション登壇も兼ねて，初めてデブサミに参加してきました．</p>

<h2>1日目</h2>

<p><a href="http://event.shoeisha.jp/devsumi/20140213/session/394/">OSSコミッタ大集合</a></p>

<p>これに登壇してました．登壇といっても，それぞれのプロジェクトの人がわいわいと自分の関わってるプロジェクトについて
交互に話すというもので，FluentdとD言語の時に壇上にあがってあれやこれや話しました．<br />
まぁFluentdに関しては登壇して何も喋らずに降りましたが…prz</p>

<p>セッションに関係ないですが登壇者控え室，和室でネットワークも電源もありで環境良すぎて超快適だった．雅叙園良いですね．</p>

<h2>2日目</h2>

<p>「Webの現在過去未来」のセッションなど気になるのがあったのですが，雪が降りすぎていたので参加しませんでした！</p>

<p>それとは別に，渋谷でRubyコミッタ3人との飲み会があり，鍋をつつきつつ色々と話をした．
その結果，Focuslightのメンテナが正式に決まり，FluentdにはLinux kernel &amp; Rubyコミッタであるkosakiさんがjoin．良い鍋でした．</p>

<blockquote class = "twitter-tweet" lang = "en"><p><a href = "https://twitter.com/sora_h">@sora_h</a> and <a href = "https://twitter.com/sonots">@sonots</a> are now focuslight committer! This is great step for growing <a href = "https://twitter.com/search?q = %23Fluentd&amp;src = hash">#Fluentd</a> visualization environment :)</p>&mdash; Mr. Fiber (@repeatedly) <a href = "https://twitter.com/repeatedly/statuses/434379434435436545">February 14, 2014</a></blockquote>


<script async src = "http://repeatedly.github.com//platform.twitter.com/widgets.js" charset = "utf-8"></script>




<blockquote class = "twitter-tweet" lang = "en"><p><a href = "https://twitter.com/kosaki55tea">@kosaki55tea</a> is now Fluentd committer! We can improve Fluentd more quickly :) <a href = "https://twitter.com/search?q = %23fluentd&amp;src = hash">#fluentd</a></p>&mdash; Mr. Fiber (@repeatedly) <a href = "https://twitter.com/repeatedly/statuses/434395924224167936">February 14, 2014</a></blockquote>


<script async src = "http://repeatedly.github.com//platform.twitter.com/widgets.js" charset = "utf-8"></script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Analyze event logs using Fluentd and Elasticsearch]]></title>
    <link href="http://repeatedly.github.com/2014/02/analyze-event-logs-using-fluentd-and-elasticsearch/"/>
    <updated>2014-02-12T17:25:00+09:00</updated>
    <id>http://repeatedly.github.com/2014/02/analyze-event-logs-using-fluentd-and-elasticsearch</id>
    <content type="html"><![CDATA[<p><a href="http://fluentd.org/">Fluentd</a> is a flexible and robust event log collector, but Fluentd doesn&#8217;t have own data-store and Web UI.
If you want to analyze the event logs collected by Fluentd, then you can use <a href="http://www.elasticsearch.org/">Elasticsearch</a> and <a href="http://www.elasticsearch.org/overview/kibana/">Kibana</a> :)</p>

<p>Elasticsearch is an easy to use Distributed Search Engine and Kibana is an awesome Web front-end for Elasticsearch.</p>

<h2>Setup</h2>

<p>I tested on Mac OS X.</p>

<h3>Pre requirements</h3>

<ul>
<li>Java for Elasticsearch</li>
</ul>


<p>Use Mac OS X&#8217;s Java.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>% java -version
</span><span class='line'>java version <span class="s2">&quot;1.6.0_51&quot;</span>
</span><span class='line'>Java<span class="o">(</span>TM<span class="o">)</span> SE Runtime Environment <span class="o">(</span>build 1.6.0_51-b11-457-11M4509<span class="o">)</span>
</span><span class='line'>Java HotSpot<span class="o">(</span>TM<span class="o">)</span> 64-Bit Server VM <span class="o">(</span>build 20.51-b01-457, mixed mode<span class="o">)</span>
</span></code></pre></td></tr></table></div></figure>


<ul>
<li>Ruby for Fluentd</li>
</ul>


<p>In this article, I use my rbenv&#8217;s Ruby and Fluentd gem directly.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>% ruby --version
</span><span class='line'>ruby 1.9.3p484 <span class="o">(</span>2013-11-22 revision 43786<span class="o">)</span> <span class="o">[</span>x86_64-darwin12.4.0<span class="o">]</span>
</span></code></pre></td></tr></table></div></figure>


<p>Note that Fluentd doesn&#8217;t work on Ruby 1.8.</p>

<h4>Treasure Agent (td-agent)</h4>

<p>Treasure Agent, as known as <code>td-agent</code>, is a stable distribution which consists of Fluentd, popular plugins and own Ruby processor.
Many users use <code>td-agent</code> instead of Fluentd gem. See <a href="http://docs.fluentd.org/articles/faq#treasure-agenttd-agnt">this FAQ</a>.</p>

<h3>Elasticsearch</h3>

<p>Downlod and extract the latest package.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>% curl -O https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.11.tar.gz
</span><span class='line'>% tar zxvf elasticsearch-0.90.11.tar.gz
</span><span class='line'>% <span class="nb">cd </span>elasticsearch-0.90.11/
</span></code></pre></td></tr></table></div></figure>


<p>I use version v0.90.11 but <a href="http://www.elasticsearch.org/blog/0-90-11-1-0-0-rc2-released/">v1.0 will be released</a> soon!</p>

<p>Start Elasticsearch:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>% ./bin/elasticsearch -f
</span><span class='line'><span class="o">[</span>2014-02-12 17:51:32,645<span class="o">][</span>INFO <span class="o">][</span>node                     <span class="o">]</span> <span class="o">[</span>Powerhouse<span class="o">]</span> version<span class="o">[</span>0.90.11<span class="o">]</span>, pid<span class="o">[</span>79737<span class="o">]</span>, build<span class="o">[</span>11da1ba/2014-02-03T15:27:39Z<span class="o">]</span>
</span><span class='line'><span class="o">[</span>2014-02-12 17:51:32,646<span class="o">][</span>INFO <span class="o">][</span>node                     <span class="o">]</span> <span class="o">[</span>Powerhouse<span class="o">]</span> initializing ...
</span><span class='line'><span class="o">[</span>2014-02-12 17:51:32,651<span class="o">][</span>INFO <span class="o">][</span>plugins                  <span class="o">]</span> <span class="o">[</span>Powerhouse<span class="o">]</span> loaded <span class="o">[]</span>, sites <span class="o">[]</span>
</span><span class='line'><span class="o">[</span>2014-02-12 17:51:34,319<span class="o">][</span>INFO <span class="o">][</span>node                     <span class="o">]</span> <span class="o">[</span>Powerhouse<span class="o">]</span> initialized
</span><span class='line'><span class="o">[</span>2014-02-12 17:51:34,320<span class="o">][</span>INFO <span class="o">][</span>node                     <span class="o">]</span> <span class="o">[</span>Powerhouse<span class="o">]</span> starting ...
</span><span class='line'><span class="o">[</span>2014-02-12 17:51:34,395<span class="o">][</span>INFO <span class="o">][</span>transport                <span class="o">]</span> <span class="o">[</span>Powerhouse<span class="o">]</span> bound_address <span class="o">{</span> inet<span class="o">[</span>/0:0:0:0:0:0:0:0%0:9300<span class="o">]}</span>, publish_address <span class="o">{</span> inet<span class="o">[</span>/192.168.1.112:9300<span class="o">]}</span>
</span><span class='line'><span class="o">[</span>2014-02-12 17:51:37,448<span class="o">][</span>INFO <span class="o">][</span>cluster.service          <span class="o">]</span> <span class="o">[</span>Powerhouse<span class="o">]</span> new_master <span class="o">[</span>Powerhouse<span class="o">][</span>tL1IC8xHSCudeVsFt4JFsQ<span class="o">][</span>inet<span class="o">[</span>/192.168.1.112:9300<span class="o">]]</span>, reason: zen-disco-join <span class="o">(</span>elected_as_master<span class="o">)</span>
</span><span class='line'><span class="o">[</span>2014-02-12 17:51:37,481<span class="o">][</span>INFO <span class="o">][</span>discovery                <span class="o">]</span> <span class="o">[</span>Powerhouse<span class="o">]</span> elasticsearch/tL1IC8xHSCudeVsFt4JFsQ
</span><span class='line'><span class="o">[</span>2014-02-12 17:51:37,492<span class="o">][</span>INFO <span class="o">][</span>http                     <span class="o">]</span> <span class="o">[</span>Powerhouse<span class="o">]</span> bound_address <span class="o">{</span> inet<span class="o">[</span>/0:0:0:0:0:0:0:0%0:9200<span class="o">]}</span>, publish_address <span class="o">{</span> inet<span class="o">[</span>/192.168.1.112:9200<span class="o">]}</span>
</span><span class='line'><span class="o">[</span>2014-02-12 17:51:37,493<span class="o">][</span>INFO <span class="o">][</span>node                     <span class="o">]</span> <span class="o">[</span>Powerhouse<span class="o">]</span> started
</span><span class='line'><span class="o">[</span>2014-02-12 17:51:37,505<span class="o">][</span>INFO <span class="o">][</span>gateway                  <span class="o">]</span> <span class="o">[</span>Powerhouse<span class="o">]</span> recovered <span class="o">[</span>0<span class="o">]</span> indices into cluster_state
</span></code></pre></td></tr></table></div></figure>


<h3>Kibana</h3>

<p>Latest Kibana, called Kibana 3, consists of only HTML and JavaScript,
so setup is very easy:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>% curl -O https://download.elasticsearch.org/kibana/kibana/kibana-3.0.0milestone5.tar.gz
</span><span class='line'>% tar zxvf kibana-3.0.0milestone5.tar.gz
</span><span class='line'>% <span class="nb">cd </span>kibana-3.0.0milestone5/
</span></code></pre></td></tr></table></div></figure>


<p>Open index.html in the kibana directory:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>% open index.html
</span></code></pre></td></tr></table></div></figure>


<p>If you want to change Kibana configuration, please edit <code>config.js</code>, e.g. change elasticsearch URL.</p>

<h3>Fluentd</h3>

<p>Install Fluentd gem and <a href="https://github.com/uken/fluent-plugin-elasticsearch">Elasticsearch plugin</a>:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>% gem install fluentd fluent-plugin-elasticsearch
</span></code></pre></td></tr></table></div></figure>


<p>Fluentd configuration is below:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='apache'><span class='line'><span class="c"># es.conf</span>
</span><span class='line'><span class="nt">&lt;source&gt;</span>
</span><span class='line'>  <span class="nb">type</span> forward
</span><span class='line'><span class="nt">&lt;/source&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;match</span> <span class="s">es.**</span><span class="nt">&gt;</span>
</span><span class='line'>  <span class="nb">type</span> elasticsearch
</span><span class='line'>  <span class="err">logstash_</span><span class="nb">format</span> true
</span><span class='line'>  <span class="err">flush_</span><span class="nb">interval</span> <span class="m">5</span>s # <span class="m">5</span>s for testing. <span class="k">On</span> production environment, <span class="m">60</span>s or higher is better
</span><span class='line'><span class="nt">&lt;/match&gt;</span>
</span></code></pre></td></tr></table></div></figure>


<p>fluent-plugin-elasticsearch provides <code>logstash_format</code> option.
It enables Kibana to analyze the event logs with day based indexes.</p>

<p>Start Fluentd:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>% fluentd -c es.conf
</span><span class='line'>2014-02-12 18:43:31 +0900 <span class="o">[</span>info<span class="o">]</span>: starting fluentd-0.10.43
</span><span class='line'>2014-02-12 18:43:31 +0900 <span class="o">[</span>info<span class="o">]</span>: reading config file <span class="nv">path</span> <span class="o">=</span> <span class="s2">&quot;es.conf&quot;</span>
</span><span class='line'>2014-02-12 18:43:32 +0900 <span class="o">[</span>info<span class="o">]</span>: gem <span class="s1">&#39;fluent-plugin-elasticsearch&#39;</span> version <span class="s1">&#39;0.2.0&#39;</span>
</span><span class='line'>2014-02-12 18:43:32 +0900 <span class="o">[</span>info<span class="o">]</span>: gem <span class="s1">&#39;fluentd&#39;</span> version <span class="s1">&#39;0.10.43&#39;</span>
</span><span class='line'>2014-02-12 18:43:32 +0900 <span class="o">[</span>info<span class="o">]</span>: using configuration file: &lt;ROOT&gt;
</span><span class='line'>  &lt;<span class="nb">source</span>&gt;
</span><span class='line'>    <span class="nb">type </span>forward
</span><span class='line'>  &lt;/source&gt;
</span><span class='line'>  &lt;match es.**&gt;
</span><span class='line'>    <span class="nb">type </span>elasticsearch
</span><span class='line'>    logstash_format <span class="nb">true</span>
</span><span class='line'><span class="nb">    </span>flush_interval 5s
</span><span class='line'>  &lt;/match&gt;
</span><span class='line'>&lt;/ROOT&gt;
</span><span class='line'>2014-02-12 18:43:32 +0900 <span class="o">[</span>info<span class="o">]</span>: adding <span class="nb">source type</span> <span class="o">=</span> <span class="s2">&quot;forward&quot;</span>
</span><span class='line'>2014-02-12 18:43:32 +0900 <span class="o">[</span>info<span class="o">]</span>: adding match <span class="nv">pattern</span> <span class="o">=</span> <span class="s2">&quot;es.**&quot;</span> <span class="nb">type</span> <span class="o">=</span> <span class="s2">&quot;elasticsearch&quot;</span>
</span><span class='line'>2014-02-12 18:43:32 +0900 <span class="o">[</span>info<span class="o">]</span>: listening fluent socket on 0.0.0.0:24224
</span></code></pre></td></tr></table></div></figure>


<h2>Analyze event logs</h2>

<p>Send some events to Fluentd. <code>fluent-cat</code> is an utility command to send json text to Fluentd&#8217;s <code>in_forward</code> plugin.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'>% <span class="nb">echo</span> <span class="s1">&#39;{&quot;message&quot;:&quot;D&quot;}&#39;</span> | fluent-cat es.event <span class="c"># es.event is a tag. es.event matches es.** of &lt;match&gt;</span>
</span><span class='line'>% <span class="nb">echo</span> <span class="s1">&#39;{&quot;message&quot;:&quot;Ruby&quot;}&#39;</span> | fluent-cat es.event
</span><span class='line'>% <span class="nb">echo</span> <span class="s1">&#39;{&quot;message&quot;:&quot;Elasticsearch&quot;}&#39;</span> | fluent-cat es.event
</span><span class='line'>% <span class="nb">echo</span> <span class="s1">&#39;{&quot;message&quot;:&quot;msgpack&quot;}&#39;</span> | fluent-cat es.event
</span><span class='line'>% ...
</span></code></pre></td></tr></table></div></figure>


<p>After Fluentd flushed received events to Elasticsearch, you can analyze the event logs via Kibana!
Following image is one panel example:</p>

<p><img src="http://repeatedly.github.com/images/fluentd_kibana_elasticsearch.png" title="&#34;Fluentd, Elasticsearch and Kibana&#34;" alt="&#34;Fluentd, Elasticsearch and Kibana&#34;"></p>

<p>Kibana has some built-in panels, so you can create own dashboard easily. See <a href="http://demo.kibana.org/#/dashboard">Kibana demo</a></p>

<h2>Advanced tips</h2>

<p>If your service has a high traffic, then fluent-plugin-elasticsearch sometimes get stucked.
In this case, built-in <a href="http://docs.fluentd.org/articles/out_roundrobin">out_roundrobin plugin</a> is useful.
You can distribute a write request to elasticsearch nodes for load balancing.</p>

<p>Of course, putting Queue with multiple fluentd nodes is an another approach.</p>

<h2>Conclusion</h2>

<p>This article introduced Fluentd, Elasticsearch and Kibana combination to analyze the event logs.
These components are easy to setup and work fine, so you can try this framework soon!
I heard many companies have already tried / deployed this setup on production :)</p>

<p>Enjoy Fluentd and Elasticsearch!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[高トラフィックでのFluentdからElasticsearchへの書き込み問題への対策]]></title>
    <link href="http://repeatedly.github.com/ja/2014/02/measures-for-fluentd-with-elasticsearch-issue-on-high-traffic/"/>
    <updated>2014-02-11T18:39:00+09:00</updated>
    <id>http://repeatedly.github.com/ja/2014/02/measures-for-fluentd-with-elasticsearch-issue-on-high-traffic</id>
    <content type="html"><![CDATA[<p><a href="http://blog.kakipo.com/trouble-with-fluentd-and-elasticsearch/">Fluentd -> Elasticsearch 大量データ転送でトラブル</a></p>

<p>上の記事にあるように，Elasticsearchに大量のデータを一気に流し込むと色々と問題が起きます．
元々検索エンジンはスケールさせるのが難しく，よく当たる問題だと思います．</p>

<p>また，Fluentdとかだとガンガンログを流し込むことも多く，この辺で詰まる云々はたまに聞きます．</p>

<p><a href="http://elasticsearch.doorkeeper.jp/events/7491">第3回elasticsearch勉強会</a></p>

<p>で，Elasticsearch勉強会にFlorianという本家のエンジニアが来ていたので，
懇親会でこの辺どうすればいいのか聞いてみました．
実際Elasticsearchユーザの中でもちょくちょく問題になるらしく，
大きくわけて二つの方法(またはこの組み合わせ)で回避しているようです．</p>

<h2>書き込み先のノードを増やす</h2>

<p>1ノードへの書き込みで詰まるなら，もっとノードを増やせば良いというアプローチ．
今のfluent-plugin-elasticsearchにはラウンドロビンの機能はないんですが，
out_roundrobinとかと組み合わせてやれば，書き込みを複数台に散らせることは出来そうです．</p>

<h2>前段にキューを置く</h2>

<p>書き込みをするノードの前にキューを置いて，そこでバッファリングしておくという方法．
Fluentdの場合はバッファがキューになっているので，そこのフラッシュ間隔を長めに調整するとか，
キュー系のプラグインがいくつかあるので，それを間に挟むという感じになるのかなと．<br />
ただ，キューから値をポップする間隔が短いと結局詰まるので，この辺は少し工夫が必要そうです．</p>

<h2>まとめ</h2>

<p>Elasticsearchを真面目に運用したことがないので，どっちが現実的なのかよく分からないのが正直な所．
この辺は高トラフィック環境で使っている人の話を聞きたい所です．</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Prestoソースコードリーディング #1]]></title>
    <link href="http://repeatedly.github.com/ja/2014/02/presto-scr-1/"/>
    <updated>2014-02-11T02:39:00+09:00</updated>
    <id>http://repeatedly.github.com/ja/2014/02/presto-scr-1</id>
    <content type="html"><![CDATA[<p><a href="http://atnd.org/events/47149">Presto ソースコードリーディング #1のATND</a></p>

<p>@tagomorisさんに場所を確保してもらって，LINEで第一回をやりました．</p>

<h2>開催の流れ</h2>

<p>CROSS辺りでPrestoのソースコードリーディングしたいね，という話が出て，
じゃあ俺が立てるので場所はLINE辺りで〜というその場のノリで決まった．</p>

<p>どうせ10人前後だろうということでかなり適当な感じで募集とかやってたんだけど，
応募人数が40人越え，当日参加が25人前後くらいだったので，予想より多かったかなという感じ．</p>

<h2>やったこと</h2>

<h3>全体の概要</h3>

<p>俺が軽めにやりました．Prestoの生まれた背景とか，依存している主要なライブラリ，
@frsyukiのスライドを拝借しての主要なクラス群の紹介，現在サポートしている型，
Slice使ってのデータの持ち方とか．後，最近の変更周りも少し紹介した</p>

<h3>HTTPレイヤー</h3>

<p>@tagomorisさんが担当で，クライアントが最初に到達するHTTP APIの <code>/v1/execution</code> や <code>/v1/statement</code> 周りの
処理の話．ソースコード的にはpresto-server．この辺もairliftにがっつり依存しているので，
パフォーマンスが気になるなど真面目に読みたい人はairliftも読みましょう．</p>

<h3>Execution</h3>

<p><a href="https://gist.github.com/oza/8912147">当日のgist</a></p>

<p>@oza_x86が担当．Coordinator内部でPlanやStageが出来るまでの流れ，
Workerで処理がどう進むかとかその辺の話．SplitsやそれぞれのRunnerの処理，
時間が掛かっているタスクの優先度下げるとか，工夫がいくつか見て取れた感じ．</p>

<h2>感想</h2>

<p>適当にやったわりにはそれなりに上手く進んで良かった．</p>

<p>第二回は，shot6さん，ashigeruさん，feeblefakieさん辺りが担当になってくれるかも？
という感じです．またソース読みつつあーだこーだ言い合いましょう．</p>
]]></content>
  </entry>
  
</feed>
